{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;;\" src='Figures/alinco.png' /></a>\n",
    "\n",
    "# Modulo I: Construyendo y visualizando frecuencias de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este apartado, nos centraremos en la funci√≥n auxiliar `build_freqs()` y en la visualizaci√≥n de un conjunto de datos introducido en ella. En nuestro objetivo de an√°lisis de sentimiento de tweets, esta funci√≥n construir√° un diccionario donde podemos buscar cu√°ntas veces aparece una palabra en las listas de tweets positivos o negativos. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Iniciamos importando las librer√≠as"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk                                # Plibrer√≠a para NLP\n",
    "from nltk.corpus import twitter_samples    # Ejemplo de conjunto de datos de Twitter de NLTK\n",
    "import matplotlib.pyplot as plt            # biblioteca para visualizaci√≥n\n",
    "import numpy as np                         "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Importaremos algunas funciones auxiliares que proporcionamos en el archivo utils.py:\n",
    "\n",
    "* `process_tweet()`: Limpia el texto, lo convierte en token en palabras separadas, elimina las palabras vac√≠as y convierte las palabras en ra√≠ces.\n",
    "* `build_freqs()`: Cuenta la frecuencia con la que una palabra del \"corpus\" (el conjunto completo de tweets) se asoci√≥ con una etiqueta positiva \"1\" o una etiqueta negativa \"0\". Luego construye el diccionario `freqs`, donde cada clave es una tupla` (palabra, etiqueta) `, y el valor es el recuento de su frecuencia dentro del corpus de tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/gaddiel/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# download the stopwords for the process_tweet function\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# import our convenience functions\n",
    "from utils import process_tweet, build_freqs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cargar el conjunto de datos de muestra NLTK\n",
    "\n",
    "Utilizaremos la [base de datos de tweeter de NLTK](http://www.nltk.org/howto/twitter.html#Using-a-Tweet-Corpus)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tweets:  10000\n"
     ]
    }
   ],
   "source": [
    "# select the lists of positive and negative tweets\n",
    "all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "all_negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "\n",
    "# concatenate the lists, 1st part is the positive tweets followed by the negative\n",
    "tweets = all_positive_tweets + all_negative_tweets\n",
    "\n",
    "# let's see how many tweets we have\n",
    "print(\"Number of tweets: \", len(tweets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuaci√≥n, crearemos una matriz de etiquetas que coincida con los sentimientos de nuestros tweets. Este tipo de datos funciona de forma muy similar a una lista normal, pero est√° optimizado para c√°lculos y manipulaci√≥n. La matriz `labels` estar√° compuesta por 10000 elementos. Los primeros 5000 se llenar√°n con etiquetas \"1\" que denotan sentimientos positivos, y los siguientes 5000 ser√°n etiquetas \"0\" que denotan lo contrario. Podemos hacer esto f√°cilmente con una serie de operaciones proporcionadas por la biblioteca `numpy`:\n",
    "\n",
    "* `np.ones()` \n",
    "* `np.zeros()` \n",
    "* `np.append()` - concatena arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a numpy array representing labels of the tweets\n",
    "labels = np.append(np.ones((len(all_positive_tweets))), np.zeros((len(all_negative_tweets))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recordatorio de uso de dicionarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dictionary = {'key1': 1, 'key2': 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'key1': 0, 'key2': 2, 'key3': -5}\n"
     ]
    }
   ],
   "source": [
    "# Add a new entry\n",
    "dictionary['key3'] = -5\n",
    "\n",
    "# Overwrite the value of key1\n",
    "dictionary['key1'] = 0\n",
    "\n",
    "print(dictionary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing values and lookup keys\n",
    "\n",
    "Performing dictionary lookups and retrieval are common tasks in NLP. There are two ways to do this: \n",
    "\n",
    "* Using square bracket notation: This form is allowed if the lookup key is in the dictionary. It produces an error otherwise.\n",
    "* Using the [get()](https://docs.python.org/3/library/stdtypes.html#dict.get) method: This allows us to set a default value if the dictionary key does not exist. \n",
    "\n",
    "Let us see these in action:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using a square bracket lookup, it is common to use an if-else block to check for containment first (with the keyword `in`) before getting the item. On the other hand, you can use the `.get()` method if you want to set a default value when the key is not found. Let's compare these in the cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "item found:  0\n",
      "item found:  0\n"
     ]
    }
   ],
   "source": [
    "# This prints a value\n",
    "if 'key1' in dictionary:\n",
    "    print(\"item found: \", dictionary['key1'])\n",
    "else:\n",
    "    print('key1 is not defined')\n",
    "\n",
    "# Same as what you get with get\n",
    "print(\"item found: \", dictionary.get('key1', -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "key does not exist!\n",
      "-1\n"
     ]
    }
   ],
   "source": [
    "# This prints a message because the key is not found\n",
    "if 'key7' in dictionary:\n",
    "    print(dictionary['key7'])\n",
    "else:\n",
    "    print('key does not exist!')\n",
    "\n",
    "# This prints -1 because the key is not found and we set the default to -1\n",
    "print(dictionary.get('key7', -1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diccionario de frecuencia de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Echemos un vistazo a la funci√≥n **build_freqs()** en **utils.py**. Esta es la funci√≥n que crea el diccionario que contiene los recuentos de palabras de cada corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def build_freqs(tweets, ys):\n",
    "    \"\"\"Build frequencies.\n",
    "    Input:\n",
    "        tweets: a list of tweets\n",
    "        ys: an m x 1 array with the sentiment label of each tweet\n",
    "            (either 0 or 1)\n",
    "    Output:\n",
    "        freqs: a dictionary mapping each (word, sentiment) pair to its\n",
    "        frequency\n",
    "    \"\"\"\n",
    "    # Convert np array to list since zip needs an iterable.\n",
    "    # The squeeze is necessary or the list ends up with one element.\n",
    "    # Also note that this is just a NOP if ys is already a list.\n",
    "    yslist = np.squeeze(ys).tolist()\n",
    "\n",
    "    # Start with an empty dictionary and populate it by looping over all tweets\n",
    "    # and over all processed words in each tweet.\n",
    "    freqs = {}\n",
    "    for y, tweet in zip(yslist, tweets):\n",
    "        for word in process_tweet(tweet):\n",
    "            pair = (word, y)\n",
    "            if pair in freqs:\n",
    "                freqs[pair] += 1\n",
    "            else:\n",
    "                freqs[pair] = 1    \n",
    "    return freqs\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como se muestra arriba, cada clave es una tupla de 2 elementos que contiene un par \"(palabra, y)\". La \"palabra\" es un elemento en un tweet procesado, mientras que \"y\" es un n√∫mero entero que representa el corpus: \"1\" para los tweets positivos y \"0\" para los tweets negativos. El valor asociado con esta clave es el n√∫mero de veces que esa palabra aparece en el corpus especificado. Por ejemplo:\n",
    "\n",
    "```\n",
    "# \"folowfriday\" aparece 25 veces en los tweets positivos\n",
    "('sigue el viernes', 1.0): 25\n",
    "\n",
    "# \"shame\" aparece 19 veces en los tweets negativos\n",
    "('shame', 0.0): 19\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora es el momento de usar el diccionario devuelto por la funci√≥n `build_freqs()`. Primero, alimentemos nuestras listas de `tweets` y` etiquetas` y luego imprimamos un informe b√°sico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create frequency dictionary\n",
    "freqs = build_freqs(tweets, labels)\n",
    "\n",
    "# check data type\n",
    "print(f'type(freqs) = {type(freqs)}')\n",
    "\n",
    "# check length of the dictionary\n",
    "print(f'len(freqs) = {len(freqs)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now print the frequency of each word depending on its class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(freqs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Desafortunadamente, esto no ayuda mucho a comprender los datos. Ser√≠a mejor visualizar este resultado para obtener mejores conocimientos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tabla de recuentos de palabras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seleccionaremos un conjunto de palabras que nos gustar√≠a visualizar. Es mejor almacenar esta informaci√≥n temporal en una tabla que sea muy f√°cil de usar m√°s adelante."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select some words to appear in the report. we will assume that each word is unique (i.e. no duplicates)\n",
    "keys = ['happi', 'merri', 'nice', 'good', 'bad', 'sad', 'mad', 'best', 'pretti',\n",
    "        '‚ù§', ':)', ':(', 'üòí', 'üò¨', 'üòÑ', 'üòç', '‚ôõ',\n",
    "        'song', 'idea', 'power', 'play', 'magnific']\n",
    "\n",
    "# list representing our table of word counts.\n",
    "# each element consist of a sublist with this pattern: [<word>, <positive_count>, <negative_count>]\n",
    "data = []\n",
    "\n",
    "# loop through our selected words\n",
    "for word in keys:\n",
    "    \n",
    "    # initialize positive and negative counts\n",
    "    pos = 0\n",
    "    neg = 0\n",
    "    \n",
    "    # retrieve number of positive counts\n",
    "    if (word, 1) in freqs:\n",
    "        pos = freqs[(word, 1)]\n",
    "        \n",
    "    # retrieve number of negative counts\n",
    "    if (word, 0) in freqs:\n",
    "        neg = freqs[(word, 0)]\n",
    "        \n",
    "    # append the word counts to the table\n",
    "    data.append([word, pos, neg])\n",
    "    \n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Luego podemos usar un diagrama de dispersi√≥n para inspeccionar esta tabla visualmente. En lugar de graficar los conteos sin procesar, lo trazaremos en la escala logar√≠tmica para tener en cuenta las amplias discrepancias entre los conteos sin procesar (por ejemplo, `:)` tiene 3568 conteos en positivo mientras que solo 2 en negativo). La l√≠nea roja marca el l√≠mite entre las √°reas positivas y negativas. Las palabras cercanas a la l√≠nea roja se pueden clasificar como neutrales."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize = (8, 8))\n",
    "\n",
    "# convert positive raw counts to logarithmic scale. we add 1 to avoid log(0)\n",
    "x = np.log([x[1] + 1 for x in data])  \n",
    "\n",
    "# do the same for the negative counts\n",
    "y = np.log([x[2] + 1 for x in data]) \n",
    "\n",
    "# Plot a dot for each pair of words\n",
    "ax.scatter(x, y)  \n",
    "\n",
    "# assign axis labels\n",
    "plt.xlabel(\"Log Positive count\")\n",
    "plt.ylabel(\"Log Negative count\")\n",
    "\n",
    "# Add the word as the label at the same position as you added the points just before\n",
    "for i in range(0, len(data)):\n",
    "    ax.annotate(data[i][0], (x[i], y[i]), fontsize=12)\n",
    "\n",
    "ax.plot([0, 9], [0, 9], color = 'red') # Plot the red line that divides the 2 areas.\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este cuadro es sencillo de interpretar. Muestra que los emoticones `:)` y `: (` son muy importantes para el an√°lisis de sentimientos. Por lo tanto, ¬°no debemos permitir que los pasos de preprocesamiento eliminen estos s√≠mbolos!\n",
    "\n",
    "¬øQu√© pasa con el s√≠mbolo de la corona?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
