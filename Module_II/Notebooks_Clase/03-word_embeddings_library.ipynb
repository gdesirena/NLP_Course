{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img style=\"float: left;;\" src='Figures/alinco.png' /></a>\n",
    "\n",
    "# Modulo II: Vectores Palabra (Word Embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-05T21:20:42.007937Z",
     "start_time": "2020-05-05T21:20:41.987638Z"
    }
   },
   "source": [
    "# Word Embeddings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivaci√≥n\n",
    "\n",
    "### El gran problema de Bag of Words\n",
    "\n",
    "Pensemos en estas 3 frases como documentos:\n",
    "\n",
    "- $doc_1$: `¬°Buen√≠simo el croissant!`\n",
    "- $doc_2$: `¬°Estuvo espectacular ese pan franc√©s!`\n",
    "- $doc_3$: `!Buen√≠sima esa pintura!`\n",
    "\n",
    "Sabemos $doc_1$ y $doc_2$ hablan de lo mismo üçûüçûüëå y que $doc_3$ üé® no tiene mucho que ver con los otros.\n",
    "\n",
    "Supongamos que queremos ver que tan similares son ambos documentos. \n",
    "\n",
    "Para esto, generamos un modelo `Bag of Words` sobre el documento. Es decir, transformamos cada palabra a un vector one-hot y luego los sumamos por documento. \n",
    "\n",
    "Adem√°s, omitimos algunas stopwords y consideramos pan frances como un solo token.\n",
    "\n",
    "$$v = \\{buen√≠sima, croissant, estuvo, espectacular, pan\\ franc√©s, pintura\\}$$\n",
    "\n",
    "Entonces, el $\\vec{doc_1}$ quedar√°:\n",
    "\n",
    "$$\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix} + \n",
    "  \\begin{bmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix} =\n",
    "  \\begin{bmatrix}1 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix}$$\n",
    "\n",
    "El $\\vec{doc_2}$ quedar√°:\n",
    "\n",
    "$$\\begin{bmatrix}0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix} + \n",
    "  \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0\\\\ 0\\end{bmatrix} + \n",
    "  \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1\\\\ 0\\end{bmatrix} = \n",
    "  \\begin{bmatrix}0 \\\\ 0 \\\\ 1 \\\\ 1 \\\\ 1\\\\ 0\\end{bmatrix}$$\n",
    "\n",
    "Y el $\\vec{doc_3}$: \n",
    "\n",
    "$$\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 0\\end{bmatrix} + \n",
    "  \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 1\\end{bmatrix} =\n",
    "  \\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0\\\\ 1\\end{bmatrix}$$\n",
    "\n",
    "\n",
    "\n",
    "**¬øCu√°l es el problema?**\n",
    "\n",
    "`buen√≠sima` $\\begin{bmatrix}1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\0\\end{bmatrix}$ y `espectacular` $ \\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\ 0 \\\\ 0\\end{bmatrix}$ representan ideas muy similares. Por otra parte, sabemos que `croissant` $\\begin{bmatrix}0 \\\\ 1 \\\\ 0 \\\\ 0 \\\\ 0 \\\\0\\end{bmatrix}$ y `pan franc√©s` $\\begin{bmatrix}0 \\\\ 0 \\\\ 0 \\\\ 0 \\\\ 1 \\\\0\\end{bmatrix}$ se refieren al mismo objeto. Pero en este modelo, estos **son totalmente distintos**. Es decir, los vectores de las palabras que `buen√≠sima` y `espectacular` son tan distintas como `croissant` y `pan franc√©s`. Esto evidentemente, repercute en la calidad de los modelos que creamos a partir de nuestro Bag of Words.\n",
    "\n",
    "Ahora, si queremos ver que documento es mas similar a otro usando distancia euclidiana, veremos que:\n",
    "\n",
    "$$d(doc_1, doc_2) = 2.236$$\n",
    "$$d(doc_1, doc_3) = 1.414$$\n",
    "\n",
    "Es decir, $doc_1$ se parece mas a $doc_3$ aunque nosotros sabemos que $doc_1$ y $doc_2$ nos est√°n diciendo lo mismo!\n",
    "\n",
    "\n",
    "Nos gustar√≠a que eso no sucediera. Que existiera alg√∫n m√©todo que nos permitiera hacer que palabras similares tengan representaciones similares. Y que con estas, representemos mejor a los documentos.\n",
    "\n",
    "\n",
    "--------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hip√≥tesis Distribucional\n",
    "\n",
    "Estamos buscando alg√∫n enfoque que nos permita representar las palabras de forma no aislada, si no como algo que adem√°s capture el significado de esta.\n",
    "\n",
    "Pensemos un poco en la **hip√≥tesis distribucional**. Esta plantea que:\n",
    "\n",
    "    \"Palabras que ocurren en contextos iguales tienden a tener significados similares.\" \n",
    "\n",
    "O equivalentemente,\n",
    "\n",
    "    \"Una palabra es caracterizada por la compa√±√≠a que esta lleva.\"\n",
    "\n",
    "Esto nos puede hacer pensar que podr√≠amos usar los contextos de las palabras para generar vectores que describan mejor dichas palabras: en otras palabras, los `Distributional Vectors`.\n",
    "\n",
    "--------------------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word-Context Matrices\n",
    "\n",
    "Es una matriz donde cada celda $(i,j)$ representa la co-ocurrencia entre una palabra objetivo/centro $w_i$ y un contexto $c_j$. El contexto son las palabras dentro de ventana de tama√±o $k$ que rodean la palabra central. \n",
    "\n",
    "Cada fila representa a una palabra a trav√©s de su contexto. Como se puede ver, ya no es un vector one-hot, si no que ahora contiene mayor informaci√≥n.\n",
    "\n",
    "El tama√±o de la matriz es el tama√±o del vocabulario $V$ al cuadrado. Es decir $|V|*|V|$.\n",
    "\n",
    "<img src=\"./Figures/distributionalSocher.png\" alt=\"Word-context matrices\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "**Problema: Creada a partir de un corpus respetable, es gigantezca**. \n",
    "\n",
    "Por ejemplo, para $|v| = 100.000$, la matriz tendr√° $\\frac{100000 * 100000 * 4}{10^9} = 40gb $.\n",
    "\n",
    "- Es caro mantenerla en memoria \n",
    "- Los clasificadores no funcionan tan bien con tantas dimensiones (ver [maldici√≥n de la dimensionalidad](https://es.wikipedia.org/wiki/Maldici%C3%B3n_de_la_dimensi%C3%B3n)).\n",
    "\n",
    "¬øHabr√° una mejor soluci√≥n?\n",
    "\n",
    "---------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word Embeddings\n",
    "\n",
    "\n",
    "La idea principal de los Word Embeddings es crear representaciones vectoriales densas y de baja dimensionalidad $(d << |V|)$ de las palabras a partir de su contexto.  Para esto, se usan distintos modelos que emplean redes neuronales *shallow* o poco profundas.\n",
    "\n",
    "Volvamos a nuestro ejemplo anterior: `buen√≠sima` y `espectacular` ocurren muchas veces en el mismo contexto, por lo que los embeddings que los representan debiesen ser muy similares... :\n",
    "\n",
    "`buen√≠sima` $\\begin{bmatrix}0.32 \\\\ 0.44 \\\\ 0.92 \\\\ .001 \\end{bmatrix}$ y `espectacular` $\\begin{bmatrix}0.30 \\\\ 0.50 \\\\ 0.92 \\\\ .002 \\end{bmatrix}$ versus `croissant`  $\\begin{bmatrix}0.77 \\\\ 0.99 \\\\ 0.004 \\\\ .1 \\end{bmatrix}$ el cu√°l es claramente distinto.\n",
    "\n",
    "\n",
    "Pero, **¬øC√≥mo capturamos el contexto dentro de nuestros vectores?**\n",
    "\n",
    "- Depender√° del modelo que utilizemos.\n",
    "\n",
    "\n",
    "##### Word2vec y Skip-gram\n",
    "\n",
    "Word2Vec es probablemente el paquete de software mas famoso para crear word embeddings. Este nos provee herramientas para crear distintos tipos de modelos, tales como `Skip-Gram` y `Continuous Bag of Word (CBOW)`. En este caso, solo veremos `Skip-Gram`.\n",
    "\n",
    "**Skip-gram** es una task auxiliar con la que crearemos nuestros embeddings. Esta consiste en que por cada palabra del dataset, predigamos las palabras de su contexto (las palabras presentes en ventana de alg√∫n tama√±o $k$).\n",
    "\n",
    "Para resolverla, usaremos una red de una sola capa oculta. Los pesos ya entrenados de esta capa ser√°n los que usaremos como embeddings.\n",
    "\n",
    "#### Detalles del Modelo\n",
    "\n",
    "- Como dijimos, el modelo ser√° una red de una sola capa. La capa oculta tendr√° una dimensi√≥n $d$ la cual nosotros determinaremos. Esta capa no tendr√° funci√≥n de activaci√≥n. Sin embargo, la de salida si, la cual ser√° una softmax.\n",
    "\n",
    "- El vector de entrada, de tama√±o $|V|$, ser√° un vector one-hot de la palabra que estemos viendo en ese momento.\n",
    "\n",
    "- La salida, tambi√©n de tama√±o $|V|$, ser√° un vector que contenga la distribuci√≥n de probabilidad de que cada palabra del vocabulario pertenezca al contexto de la palabra de entrada.\n",
    "\n",
    "- Al entrenar, se comparar√° la distribuci√≥n de los contextos con la suma de los vectores one-hot del contexto real.\n",
    "\n",
    "<img src=\"./Figures/skip_gram_net_arch.png\" alt=\"Skip Gram\" style=\"width: 600px;\"/>\n",
    "\n",
    "Nota: Esto es computacionalmente una locura. Por cada palabra de entrada, debemos calcular la probabilidad de aparici√≥n de todas las otras. Imaginen el caso de un vocabulario de 100.000 de palabras y de 10000000 oraciones...\n",
    "\n",
    "La soluci√≥n a esto es modificar la task a *Negative Sampling*. Esta transforma este problema de $|V|$ clases a uno binario.\n",
    "\n",
    "### La capa Oculta y los Embeddings\n",
    "\n",
    "Al terminar el entrenamiento, ¬øQu√© nos queda en la capa oculta?\n",
    "\n",
    "Una matriz de $v$ filas por $d$ columnas, la cual contiene lo que buscabamos: Una representaci√≥n continua de todas las palabras de nuestro vocabualrio.  \n",
    "\n",
    "**Cada fila de la matriz es un vector que contiene la representaci√≥n continua una palabra del vocabulario.**\n",
    "\n",
    "\n",
    "<img src=\"./Figures/word2vec_weight_matrix_lookup_table.png\" alt=\"Capa Oculta 1\" style=\"width: 400px;\"/>\n",
    "\n",
    "¬øC√≥mo la usamos eficientemente?\n",
    "\n",
    "Simple: usamos los mismos vectores one-hot de la entrada y las multiplicamos por la matriz:\n",
    "\n",
    "<img src=\"./Figures/matrix_mult_w_one_hot.png\" alt=\"Skip Gram\" style=\"width: 400px;\"/>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entrenar nuestros Embeddings\n",
    "\n",
    "Para entrenar nuestros embeddings, usaremos el paquete gensim. Este trae una muy buena implementaci√≥n de `word2vec`.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "from time import time\n",
    "from collections import defaultdict\n",
    "import string\n",
    "import multiprocessing\n",
    "import os\n",
    "\n",
    "# word2vec\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "from gensim.models.phrases import Phrases, Phraser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install word2vec\n",
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cargar el dataset y limpiar\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_json('Data/data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_r = dataset.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>author</th>\n",
       "      <th>author_link</th>\n",
       "      <th>title</th>\n",
       "      <th>link</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>content</th>\n",
       "      <th>tags</th>\n",
       "      <th>embedded_links</th>\n",
       "      <th>publication_datetime</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Yerko Roa</td>\n",
       "      <td>/lista/autores/yroa</td>\n",
       "      <td>Colapsa otro segmento de casa que se derrumb√≥ ...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>region-de-valparaiso</td>\n",
       "      <td>Noticia en Desarrollo  Estamos recopilando m...</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>1565778000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Valentina Gonz√°lez</td>\n",
       "      <td>/lista/autores/vgonzalez</td>\n",
       "      <td>Polic√≠a busca a mujer acusada de matar a su pa...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>region-metropolitana</td>\n",
       "      <td>Detectives de la Polic√≠a de Investigaciones ...</td>\n",
       "      <td>[#parricidio, #PDI, #Pudahuel, #Regi√≥n Metropo...</td>\n",
       "      <td>[https://media.biobiochile.cl/wp-content/uploa...</td>\n",
       "      <td>1565771820000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Felipe Delgado</td>\n",
       "      <td>/lista/autores/fdelgado</td>\n",
       "      <td>Dos detenidos en Liceo de Aplicaci√≥n: protagon...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>region-metropolitana</td>\n",
       "      <td>Dos detenidos fue el saldo de una serie de i...</td>\n",
       "      <td>[#Incendio, #Liceo de Aplicaci√≥n, #Regi√≥n Metr...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1565772480000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mat√≠as Vega</td>\n",
       "      <td>/lista/autores/mvega</td>\n",
       "      <td>Apoyo transversal: Senado aprueba en general p...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/c...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>chile</td>\n",
       "      <td>La sala del Senado aprob√≥ en general el proy...</td>\n",
       "      <td>[#Inmigraci√≥n, #Inmigrantes, #Ley, #Migraci√≥n,...</td>\n",
       "      <td>[https://media.biobiochile.cl/wp-content/uploa...</td>\n",
       "      <td>1565772720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Valentina Gonz√°lez</td>\n",
       "      <td>/lista/autores/vgonzalez</td>\n",
       "      <td>Evacuaci√≥n espont√°nea en Instituto Nacional po...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>region-metropolitana</td>\n",
       "      <td>La ma√±ana de este mi√©rcoles se produjo una e...</td>\n",
       "      <td>[#Carabineros, #FFEE, #Gases Lacrim√≥genos, #In...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1565772960000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26408</th>\n",
       "      <td>Manuel Stuardo</td>\n",
       "      <td>/lista/autores/mstuardo</td>\n",
       "      <td>Naciones Unidas abre proceso de postulaciones ...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/c...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>chile</td>\n",
       "      <td>Las Naciones Unidas abri√≥ un proceso de post...</td>\n",
       "      <td>[#cambio clim√°tico, #COP25, #Naciones Unidas, ...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1565764200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26409</th>\n",
       "      <td>Felipe Delgado</td>\n",
       "      <td>/lista/autores/fdelgado</td>\n",
       "      <td>Fernando Astengo choc√≥ en estado de ebriedad e...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>region-metropolitana</td>\n",
       "      <td>El exfutbolista Fernando Astengo protagoniz√≥...</td>\n",
       "      <td>[#Accidente, #Fernando Astengo, #Pe√±alol√©n, #R...</td>\n",
       "      <td>[https://media.biobiochile.cl/wp-content/uploa...</td>\n",
       "      <td>1565767440000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26410</th>\n",
       "      <td>Felipe Delgado</td>\n",
       "      <td>/lista/autores/fdelgado</td>\n",
       "      <td>Detuvieron a hombre que arroj√≥ combustible a u...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>region-metropolitana</td>\n",
       "      <td>Personal de Carabineros detuvo a un hombre q...</td>\n",
       "      <td>[#Indigente, #Parque Forestal, #Regi√≥n Metropo...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1565769300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26411</th>\n",
       "      <td>Nicol√°s Parra</td>\n",
       "      <td>/lista/autores/nparra</td>\n",
       "      <td>Revelan identidad de 2 de 6 v√≠ctimas fatales e...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/r...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>region-de-valparaiso</td>\n",
       "      <td>El intendente de Valpara√≠so, Jorge Mart√≠nez,...</td>\n",
       "      <td>[#derrumbe en valpara√≠so, #Regi√≥n de Valpara√≠s...</td>\n",
       "      <td>[]</td>\n",
       "      <td>1565771100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26412</th>\n",
       "      <td>Emilio Lara</td>\n",
       "      <td>/lista/autores/elara</td>\n",
       "      <td>Senado cerrar√° cuenta paralela al presupuesto ...</td>\n",
       "      <td>https://www.biobiochile.cl/noticias/nacional/c...</td>\n",
       "      <td>nacional</td>\n",
       "      <td>chile</td>\n",
       "      <td>El presidente del Senado, Jaime Quintana (PP...</td>\n",
       "      <td>[#Dipres, #Fiscal√≠a, #Hacienda, #Senado]</td>\n",
       "      <td>[https://media.biobiochile.cl/wp-content/uploa...</td>\n",
       "      <td>1565771640000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26413 rows √ó 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   author               author_link  \\\n",
       "0               Yerko Roa       /lista/autores/yroa   \n",
       "1      Valentina Gonz√°lez  /lista/autores/vgonzalez   \n",
       "2          Felipe Delgado   /lista/autores/fdelgado   \n",
       "3             Mat√≠as Vega      /lista/autores/mvega   \n",
       "4      Valentina Gonz√°lez  /lista/autores/vgonzalez   \n",
       "...                   ...                       ...   \n",
       "26408      Manuel Stuardo   /lista/autores/mstuardo   \n",
       "26409      Felipe Delgado   /lista/autores/fdelgado   \n",
       "26410      Felipe Delgado   /lista/autores/fdelgado   \n",
       "26411       Nicol√°s Parra     /lista/autores/nparra   \n",
       "26412         Emilio Lara      /lista/autores/elara   \n",
       "\n",
       "                                                   title  \\\n",
       "0      Colapsa otro segmento de casa que se derrumb√≥ ...   \n",
       "1      Polic√≠a busca a mujer acusada de matar a su pa...   \n",
       "2      Dos detenidos en Liceo de Aplicaci√≥n: protagon...   \n",
       "3      Apoyo transversal: Senado aprueba en general p...   \n",
       "4      Evacuaci√≥n espont√°nea en Instituto Nacional po...   \n",
       "...                                                  ...   \n",
       "26408  Naciones Unidas abre proceso de postulaciones ...   \n",
       "26409  Fernando Astengo choc√≥ en estado de ebriedad e...   \n",
       "26410  Detuvieron a hombre que arroj√≥ combustible a u...   \n",
       "26411  Revelan identidad de 2 de 6 v√≠ctimas fatales e...   \n",
       "26412  Senado cerrar√° cuenta paralela al presupuesto ...   \n",
       "\n",
       "                                                    link  category  \\\n",
       "0      https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
       "1      https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
       "2      https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
       "3      https://www.biobiochile.cl/noticias/nacional/c...  nacional   \n",
       "4      https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
       "...                                                  ...       ...   \n",
       "26408  https://www.biobiochile.cl/noticias/nacional/c...  nacional   \n",
       "26409  https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
       "26410  https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
       "26411  https://www.biobiochile.cl/noticias/nacional/r...  nacional   \n",
       "26412  https://www.biobiochile.cl/noticias/nacional/c...  nacional   \n",
       "\n",
       "                subcategory  \\\n",
       "0      region-de-valparaiso   \n",
       "1      region-metropolitana   \n",
       "2      region-metropolitana   \n",
       "3                     chile   \n",
       "4      region-metropolitana   \n",
       "...                     ...   \n",
       "26408                 chile   \n",
       "26409  region-metropolitana   \n",
       "26410  region-metropolitana   \n",
       "26411  region-de-valparaiso   \n",
       "26412                 chile   \n",
       "\n",
       "                                                 content  \\\n",
       "0        Noticia en Desarrollo  Estamos recopilando m...   \n",
       "1        Detectives de la Polic√≠a de Investigaciones ...   \n",
       "2        Dos detenidos fue el saldo de una serie de i...   \n",
       "3        La sala del Senado aprob√≥ en general el proy...   \n",
       "4        La ma√±ana de este mi√©rcoles se produjo una e...   \n",
       "...                                                  ...   \n",
       "26408    Las Naciones Unidas abri√≥ un proceso de post...   \n",
       "26409    El exfutbolista Fernando Astengo protagoniz√≥...   \n",
       "26410    Personal de Carabineros detuvo a un hombre q...   \n",
       "26411    El intendente de Valpara√≠so, Jorge Mart√≠nez,...   \n",
       "26412    El presidente del Senado, Jaime Quintana (PP...   \n",
       "\n",
       "                                                    tags  \\\n",
       "0                                                     []   \n",
       "1      [#parricidio, #PDI, #Pudahuel, #Regi√≥n Metropo...   \n",
       "2      [#Incendio, #Liceo de Aplicaci√≥n, #Regi√≥n Metr...   \n",
       "3      [#Inmigraci√≥n, #Inmigrantes, #Ley, #Migraci√≥n,...   \n",
       "4      [#Carabineros, #FFEE, #Gases Lacrim√≥genos, #In...   \n",
       "...                                                  ...   \n",
       "26408  [#cambio clim√°tico, #COP25, #Naciones Unidas, ...   \n",
       "26409  [#Accidente, #Fernando Astengo, #Pe√±alol√©n, #R...   \n",
       "26410  [#Indigente, #Parque Forestal, #Regi√≥n Metropo...   \n",
       "26411  [#derrumbe en valpara√≠so, #Regi√≥n de Valpara√≠s...   \n",
       "26412           [#Dipres, #Fiscal√≠a, #Hacienda, #Senado]   \n",
       "\n",
       "                                          embedded_links  publication_datetime  \n",
       "0                                                     []         1565778000000  \n",
       "1      [https://media.biobiochile.cl/wp-content/uploa...         1565771820000  \n",
       "2                                                     []         1565772480000  \n",
       "3      [https://media.biobiochile.cl/wp-content/uploa...         1565772720000  \n",
       "4                                                     []         1565772960000  \n",
       "...                                                  ...                   ...  \n",
       "26408                                                 []         1565764200000  \n",
       "26409  [https://media.biobiochile.cl/wp-content/uploa...         1565767440000  \n",
       "26410                                                 []         1565769300000  \n",
       "26411                                                 []         1565771100000  \n",
       "26412  [https://media.biobiochile.cl/wp-content/uploa...         1565771640000  \n",
       "\n",
       "[26413 rows x 10 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_r.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'  Detectives de la Polic√≠a de Investigaciones realizan peritajes para detener a una mujer de 45 a√±os, por su presunta responsabilidad en el ataque con un arma cortante contra su propio padre , lo que caus√≥ su muerte en la comuna de Pudahuel.  El hecho ocurri√≥ en calle Presidente Truman, cerca de la intersecci√≥n con Teniente Cruz, cuando, acorde a la declaraci√≥n del hijo de la v√≠ctima y hermano de la victimaria, ambos sostuvieron un enfrentamiento verbal debido a la intensi√≥n de Hernan Silva P√©rez de vender su casa .  Negocio que habr√≠a causado m√°s que molestia en su hija, Tania Silva, quien tras la discusi√≥n habr√≠a acudido a la cocina de la vivienda para volver con un cuchillo y apu√±alar a su padre.  Las primeras diligencias la realizaron carabineros de la 45¬∫ comisar√≠a, quienes tomaron declaraci√≥n al √∫nico testigo del crimen, al interior de la vivienda.  El capit√°n, Carlos Lagos, explic√≥ que la principal hip√≥tesis apunta a una discusi√≥n por el dinero de la venta de este inmueble.    Luego, fueron detectives de la Brigada de Homicidios de la PDI quienes realizaron peritajes, corroborando que la v√≠ctima muri√≥ al ser apu√±alado en una ocasi√≥n a la altura del t√≥rax. El subcomisario, Cristi√°n Tur, asegur√≥ que la presunta responsable est√° identificada.   Por esta raz√≥n es que hay diligencias de la polic√≠a enfocadas en dar con el paradero y detener a Tania Silva Herrera, de 45 a√±os, que, acorde a lo expresado por sus familiares, vivir√≠a en situaci√≥n de calle , pero que es buscada por el delito de parricidio.   Este art√≠culo describe un proceso judicial en curso  Existe la posibilidad de que los cargos sean desestimados al finalizar la investigaci√≥n, por lo cual NO se debe considerar al o los imputados como culpables hasta que la Justicia dicte sentencia en su contra. (Art√≠culo 04 del C√≥digo Procesal Penal)   '"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_r.iloc[1,6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "content = dataset['title'] + dataset['content'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(26413,)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Colapsa otro segmento de casa que se derrumb√≥ en Valpara√≠so  Noticia en Desarrollo  Estamos recopilando m√°s antecedentes sobre esta noticia, qu√©date atento a las actualizaciones.    Parte de la estructura restante de la casa que cay√≥ ayer en Valpara√≠so colaps√≥ en la ma√±ana de este mi√©rcoles.  El hecho se produjo a las 10:15 horas, en la esquina de Aldunate con Huito, donde ayer murieron seis personas.  Seg√∫n informaci√≥n preliminar, no hab√≠a rescatistas en el lugar, porque las labores se hab√≠an suspendido por el peligro de seguir trabajando. No habr√≠a lesionados.  '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "punctuation = string.punctuation + \"¬´¬ª‚Äú‚Äù‚Äò‚Äô‚Ä¶‚Äî\"\n",
    "stopwords = pd.read_csv(\"Data/spanish.txt\").values\n",
    "stopwords = Counter(stopwords.flatten().tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'actualmente': 1,\n",
       "         'adelante': 1,\n",
       "         'adem√°s': 1,\n",
       "         'afirm√≥': 1,\n",
       "         'agreg√≥': 1,\n",
       "         'ahora': 1,\n",
       "         'ah√≠': 1,\n",
       "         'al': 1,\n",
       "         'algo': 1,\n",
       "         'alguna': 1,\n",
       "         'algunas': 1,\n",
       "         'alguno': 1,\n",
       "         'algunos': 1,\n",
       "         'alg√∫n': 1,\n",
       "         'alrededor': 1,\n",
       "         'ambos': 1,\n",
       "         'ampleamos': 1,\n",
       "         'ante': 1,\n",
       "         'anterior': 1,\n",
       "         'antes': 1,\n",
       "         'apenas': 1,\n",
       "         'aproximadamente': 1,\n",
       "         'aquel': 1,\n",
       "         'aquellas': 1,\n",
       "         'aquellos': 1,\n",
       "         'aqui': 1,\n",
       "         'aqu√≠': 1,\n",
       "         'arriba': 1,\n",
       "         'asegur√≥': 1,\n",
       "         'as√≠': 1,\n",
       "         'atras': 1,\n",
       "         'aunque': 1,\n",
       "         'ayer': 1,\n",
       "         'a√±adi√≥': 1,\n",
       "         'a√∫n': 1,\n",
       "         'bajo': 1,\n",
       "         'bastante': 1,\n",
       "         'bien': 1,\n",
       "         'buen': 1,\n",
       "         'buena': 1,\n",
       "         'buenas': 1,\n",
       "         'bueno': 1,\n",
       "         'buenos': 1,\n",
       "         'cada': 1,\n",
       "         'casi': 1,\n",
       "         'cerca': 1,\n",
       "         'cierta': 1,\n",
       "         'ciertas': 1,\n",
       "         'cierto': 1,\n",
       "         'ciertos': 1,\n",
       "         'cinco': 1,\n",
       "         'coment√≥': 1,\n",
       "         'como': 1,\n",
       "         'con': 1,\n",
       "         'conocer': 1,\n",
       "         'conseguimos': 1,\n",
       "         'conseguir': 1,\n",
       "         'considera': 1,\n",
       "         'consider√≥': 1,\n",
       "         'consigo': 1,\n",
       "         'consigue': 1,\n",
       "         'consiguen': 1,\n",
       "         'consigues': 1,\n",
       "         'contra': 1,\n",
       "         'cosas': 1,\n",
       "         'creo': 1,\n",
       "         'cual': 1,\n",
       "         'cuales': 1,\n",
       "         'cualquier': 1,\n",
       "         'cuando': 1,\n",
       "         'cuanto': 1,\n",
       "         'cuatro': 1,\n",
       "         'cuenta': 1,\n",
       "         'c√≥mo': 1,\n",
       "         'da': 1,\n",
       "         'dado': 1,\n",
       "         'dan': 1,\n",
       "         'dar': 1,\n",
       "         'de': 1,\n",
       "         'debe': 1,\n",
       "         'deben': 1,\n",
       "         'debido': 1,\n",
       "         'decir': 1,\n",
       "         'dej√≥': 1,\n",
       "         'del': 1,\n",
       "         'dem√°s': 1,\n",
       "         'dentro': 1,\n",
       "         'desde': 1,\n",
       "         'despu√©s': 1,\n",
       "         'dice': 1,\n",
       "         'dicen': 1,\n",
       "         'dicho': 1,\n",
       "         'dieron': 1,\n",
       "         'diferente': 1,\n",
       "         'diferentes': 1,\n",
       "         'dijeron': 1,\n",
       "         'dijo': 1,\n",
       "         'dio': 1,\n",
       "         'donde': 1,\n",
       "         'dos': 1,\n",
       "         'durante': 1,\n",
       "         'e': 1,\n",
       "         'ejemplo': 1,\n",
       "         'el': 1,\n",
       "         'ella': 1,\n",
       "         'ellas': 1,\n",
       "         'ello': 1,\n",
       "         'ellos': 1,\n",
       "         'embargo': 1,\n",
       "         'empleais': 1,\n",
       "         'emplean': 1,\n",
       "         'emplear': 1,\n",
       "         'empleas': 1,\n",
       "         'empleo': 1,\n",
       "         'en': 1,\n",
       "         'encima': 1,\n",
       "         'encuentra': 1,\n",
       "         'entonces': 1,\n",
       "         'entre': 1,\n",
       "         'era': 1,\n",
       "         'erais': 1,\n",
       "         'eramos': 1,\n",
       "         'eran': 1,\n",
       "         'eras': 1,\n",
       "         'eres': 1,\n",
       "         'es': 1,\n",
       "         'esa': 1,\n",
       "         'esas': 1,\n",
       "         'ese': 1,\n",
       "         'eso': 1,\n",
       "         'esos': 1,\n",
       "         'esta': 1,\n",
       "         'estaba': 1,\n",
       "         'estabais': 1,\n",
       "         'estaban': 1,\n",
       "         'estabas': 1,\n",
       "         'estad': 1,\n",
       "         'estada': 1,\n",
       "         'estadas': 1,\n",
       "         'estado': 1,\n",
       "         'estados': 1,\n",
       "         'estais': 1,\n",
       "         'estamos': 1,\n",
       "         'estan': 1,\n",
       "         'estando': 1,\n",
       "         'estar': 1,\n",
       "         'estaremos': 1,\n",
       "         'estar√°': 1,\n",
       "         'estar√°n': 1,\n",
       "         'estar√°s': 1,\n",
       "         'estar√©': 1,\n",
       "         'estar√©is': 1,\n",
       "         'estar√≠a': 1,\n",
       "         'estar√≠ais': 1,\n",
       "         'estar√≠amos': 1,\n",
       "         'estar√≠an': 1,\n",
       "         'estar√≠as': 1,\n",
       "         'estas': 1,\n",
       "         'este': 1,\n",
       "         'estemos': 1,\n",
       "         'esto': 1,\n",
       "         'estos': 1,\n",
       "         'estoy': 1,\n",
       "         'estuve': 1,\n",
       "         'estuviera': 1,\n",
       "         'estuvierais': 1,\n",
       "         'estuvieran': 1,\n",
       "         'estuvieras': 1,\n",
       "         'estuvieron': 1,\n",
       "         'estuviese': 1,\n",
       "         'estuvieseis': 1,\n",
       "         'estuviesen': 1,\n",
       "         'estuvieses': 1,\n",
       "         'estuvimos': 1,\n",
       "         'estuviste': 1,\n",
       "         'estuvisteis': 1,\n",
       "         'estuvi√©ramos': 1,\n",
       "         'estuvi√©semos': 1,\n",
       "         'estuvo': 1,\n",
       "         'est√°': 1,\n",
       "         'est√°bamos': 1,\n",
       "         'est√°is': 1,\n",
       "         'est√°n': 1,\n",
       "         'est√°s': 1,\n",
       "         'est√©': 1,\n",
       "         'est√©is': 1,\n",
       "         'est√©n': 1,\n",
       "         'est√©s': 1,\n",
       "         'ex': 1,\n",
       "         'existe': 1,\n",
       "         'existen': 1,\n",
       "         'explic√≥': 1,\n",
       "         'expres√≥': 1,\n",
       "         'fin': 1,\n",
       "         'fue': 1,\n",
       "         'fuera': 1,\n",
       "         'fuerais': 1,\n",
       "         'fueran': 1,\n",
       "         'fueras': 1,\n",
       "         'fueron': 1,\n",
       "         'fuese': 1,\n",
       "         'fueseis': 1,\n",
       "         'fuesen': 1,\n",
       "         'fueses': 1,\n",
       "         'fui': 1,\n",
       "         'fuimos': 1,\n",
       "         'fuiste': 1,\n",
       "         'fuisteis': 1,\n",
       "         'fu√©ramos': 1,\n",
       "         'fu√©semos': 1,\n",
       "         'gran': 1,\n",
       "         'grandes': 1,\n",
       "         'gueno': 1,\n",
       "         'ha': 1,\n",
       "         'haber': 1,\n",
       "         'habida': 1,\n",
       "         'habidas': 1,\n",
       "         'habido': 1,\n",
       "         'habidos': 1,\n",
       "         'habiendo': 1,\n",
       "         'habremos': 1,\n",
       "         'habr√°': 1,\n",
       "         'habr√°n': 1,\n",
       "         'habr√°s': 1,\n",
       "         'habr√©': 1,\n",
       "         'habr√©is': 1,\n",
       "         'habr√≠a': 1,\n",
       "         'habr√≠ais': 1,\n",
       "         'habr√≠amos': 1,\n",
       "         'habr√≠an': 1,\n",
       "         'habr√≠as': 1,\n",
       "         'hab√©is': 1,\n",
       "         'hab√≠a': 1,\n",
       "         'hab√≠ais': 1,\n",
       "         'hab√≠amos': 1,\n",
       "         'hab√≠an': 1,\n",
       "         'hab√≠as': 1,\n",
       "         'hace': 1,\n",
       "         'haceis': 1,\n",
       "         'hacemos': 1,\n",
       "         'hacen': 1,\n",
       "         'hacer': 1,\n",
       "         'hacerlo': 1,\n",
       "         'haces': 1,\n",
       "         'hacia': 1,\n",
       "         'haciendo': 1,\n",
       "         'hago': 1,\n",
       "         'han': 1,\n",
       "         'has': 1,\n",
       "         'hasta': 1,\n",
       "         'hay': 1,\n",
       "         'haya': 1,\n",
       "         'hayamos': 1,\n",
       "         'hayan': 1,\n",
       "         'hayas': 1,\n",
       "         'hay√°is': 1,\n",
       "         'he': 1,\n",
       "         'hecho': 1,\n",
       "         'hemos': 1,\n",
       "         'hicieron': 1,\n",
       "         'hizo': 1,\n",
       "         'hoy': 1,\n",
       "         'hube': 1,\n",
       "         'hubiera': 1,\n",
       "         'hubierais': 1,\n",
       "         'hubieran': 1,\n",
       "         'hubieras': 1,\n",
       "         'hubieron': 1,\n",
       "         'hubiese': 1,\n",
       "         'hubieseis': 1,\n",
       "         'hubiesen': 1,\n",
       "         'hubieses': 1,\n",
       "         'hubimos': 1,\n",
       "         'hubiste': 1,\n",
       "         'hubisteis': 1,\n",
       "         'hubi√©ramos': 1,\n",
       "         'hubi√©semos': 1,\n",
       "         'hubo': 1,\n",
       "         'igual': 1,\n",
       "         'incluso': 1,\n",
       "         'indic√≥': 1,\n",
       "         'inform√≥': 1,\n",
       "         'intenta': 1,\n",
       "         'intentais': 1,\n",
       "         'intentamos': 1,\n",
       "         'intentan': 1,\n",
       "         'intentar': 1,\n",
       "         'intentas': 1,\n",
       "         'intento': 1,\n",
       "         'ir': 1,\n",
       "         'junto': 1,\n",
       "         'la': 1,\n",
       "         'lado': 1,\n",
       "         'largo': 1,\n",
       "         'las': 1,\n",
       "         'le': 1,\n",
       "         'les': 1,\n",
       "         'lleg√≥': 1,\n",
       "         'lleva': 1,\n",
       "         'llevar': 1,\n",
       "         'lo': 1,\n",
       "         'los': 1,\n",
       "         'luego': 1,\n",
       "         'lugar': 1,\n",
       "         'manera': 1,\n",
       "         'manifest√≥': 1,\n",
       "         'mayor': 1,\n",
       "         'me': 1,\n",
       "         'mediante': 1,\n",
       "         'mejor': 1,\n",
       "         'mencion√≥': 1,\n",
       "         'menos': 1,\n",
       "         'mi': 1,\n",
       "         'mientras': 1,\n",
       "         'mio': 1,\n",
       "         'mis': 1,\n",
       "         'misma': 1,\n",
       "         'mismas': 1,\n",
       "         'mismo': 1,\n",
       "         'mismos': 1,\n",
       "         'modo': 1,\n",
       "         'momento': 1,\n",
       "         'mucha': 1,\n",
       "         'muchas': 1,\n",
       "         'mucho': 1,\n",
       "         'muchos': 1,\n",
       "         'muy': 1,\n",
       "         'm√°s': 1,\n",
       "         'm√≠': 1,\n",
       "         'm√≠a': 1,\n",
       "         'm√≠as': 1,\n",
       "         'm√≠o': 1,\n",
       "         'm√≠os': 1,\n",
       "         'nada': 1,\n",
       "         'nadie': 1,\n",
       "         'ni': 1,\n",
       "         'ninguna': 1,\n",
       "         'ningunas': 1,\n",
       "         'ninguno': 1,\n",
       "         'ningunos': 1,\n",
       "         'ning√∫n': 1,\n",
       "         'no': 1,\n",
       "         'nos': 1,\n",
       "         'nosotras': 1,\n",
       "         'nosotros': 1,\n",
       "         'nuestra': 1,\n",
       "         'nuestras': 1,\n",
       "         'nuestro': 1,\n",
       "         'nuestros': 1,\n",
       "         'nueva': 1,\n",
       "         'nuevas': 1,\n",
       "         'nuevo': 1,\n",
       "         'nuevos': 1,\n",
       "         'nunca': 1,\n",
       "         'o': 1,\n",
       "         'ocho': 1,\n",
       "         'os': 1,\n",
       "         'otra': 1,\n",
       "         'otras': 1,\n",
       "         'otro': 1,\n",
       "         'otros': 1,\n",
       "         'para': 1,\n",
       "         'parece': 1,\n",
       "         'parte': 1,\n",
       "         'partir': 1,\n",
       "         'pasada': 1,\n",
       "         'pasado': 1,\n",
       "         'pero': 1,\n",
       "         'pesar': 1,\n",
       "         'poca': 1,\n",
       "         'pocas': 1,\n",
       "         'poco': 1,\n",
       "         'pocos': 1,\n",
       "         'podeis': 1,\n",
       "         'podemos': 1,\n",
       "         'poder': 1,\n",
       "         'podria': 1,\n",
       "         'podriais': 1,\n",
       "         'podriamos': 1,\n",
       "         'podrian': 1,\n",
       "         'podrias': 1,\n",
       "         'podr√°': 1,\n",
       "         'podr√°n': 1,\n",
       "         'podr√≠a': 1,\n",
       "         'podr√≠an': 1,\n",
       "         'poner': 1,\n",
       "         'por': 1,\n",
       "         'por qu√©': 1,\n",
       "         'porque': 1,\n",
       "         'posible': 1,\n",
       "         'primer': 1,\n",
       "         'primera': 1,\n",
       "         'primero': 1,\n",
       "         'primeros': 1,\n",
       "         'principalmente': 1,\n",
       "         'propia': 1,\n",
       "         'propias': 1,\n",
       "         'propio': 1,\n",
       "         'propios': 1,\n",
       "         'pr√≥ximo': 1,\n",
       "         'pr√≥ximos': 1,\n",
       "         'pudo': 1,\n",
       "         'pueda': 1,\n",
       "         'puede': 1,\n",
       "         'pueden': 1,\n",
       "         'puedo': 1,\n",
       "         'pues': 1,\n",
       "         'que': 1,\n",
       "         'qued√≥': 1,\n",
       "         'queremos': 1,\n",
       "         'quien': 1,\n",
       "         'quienes': 1,\n",
       "         'quiere': 1,\n",
       "         'qui√©n': 1,\n",
       "         'qu√©': 1,\n",
       "         'realizado': 1,\n",
       "         'realizar': 1,\n",
       "         'realiz√≥': 1,\n",
       "         'respecto': 1,\n",
       "         'sabe': 1,\n",
       "         'sabeis': 1,\n",
       "         'sabemos': 1,\n",
       "         'saben': 1,\n",
       "         'saber': 1,\n",
       "         'sabes': 1,\n",
       "         'se': 1,\n",
       "         'sea': 1,\n",
       "         'seamos': 1,\n",
       "         'sean': 1,\n",
       "         'seas': 1,\n",
       "         'segunda': 1,\n",
       "         'segundo': 1,\n",
       "         'seg√∫n': 1,\n",
       "         'seis': 1,\n",
       "         'ser': 1,\n",
       "         'seremos': 1,\n",
       "         'ser√°': 1,\n",
       "         'ser√°n': 1,\n",
       "         'ser√°s': 1,\n",
       "         'ser√©': 1,\n",
       "         'ser√©is': 1,\n",
       "         'ser√≠a': 1,\n",
       "         'ser√≠ais': 1,\n",
       "         'ser√≠amos': 1,\n",
       "         'ser√≠an': 1,\n",
       "         'ser√≠as': 1,\n",
       "         'se√°is': 1,\n",
       "         'se√±al√≥': 1,\n",
       "         'si': 1,\n",
       "         'sido': 1,\n",
       "         'siempre': 1,\n",
       "         'siendo': 1,\n",
       "         'siete': 1,\n",
       "         'sigue': 1,\n",
       "         'siguiente': 1,\n",
       "         'sin': 1,\n",
       "         'sino': 1,\n",
       "         'sobre': 1,\n",
       "         'sois': 1,\n",
       "         'sola': 1,\n",
       "         'solamente': 1,\n",
       "         'solas': 1,\n",
       "         'solo': 1,\n",
       "         'solos': 1,\n",
       "         'somos': 1,\n",
       "         'son': 1,\n",
       "         'soy': 1,\n",
       "         'su': 1,\n",
       "         'sus': 1,\n",
       "         'suya': 1,\n",
       "         'suyas': 1,\n",
       "         'suyo': 1,\n",
       "         'suyos': 1,\n",
       "         's√≠': 1,\n",
       "         's√≥lo': 1,\n",
       "         'tal': 1,\n",
       "         'tambi√©n': 1,\n",
       "         'tampoco': 1,\n",
       "         'tan': 1,\n",
       "         'tanto': 1,\n",
       "         'te': 1,\n",
       "         'tendremos': 1,\n",
       "         'tendr√°': 1,\n",
       "         'tendr√°n': 1,\n",
       "         'tendr√°s': 1,\n",
       "         'tendr√©': 1,\n",
       "         'tendr√©is': 1,\n",
       "         'tendr√≠a': 1,\n",
       "         'tendr√≠ais': 1,\n",
       "         'tendr√≠amos': 1,\n",
       "         'tendr√≠an': 1,\n",
       "         'tendr√≠as': 1,\n",
       "         'tened': 1,\n",
       "         'teneis': 1,\n",
       "         'tenemos': 1,\n",
       "         'tener': 1,\n",
       "         'tenga': 1,\n",
       "         'tengamos': 1,\n",
       "         'tengan': 1,\n",
       "         'tengas': 1,\n",
       "         'tengo': 1,\n",
       "         'teng√°is': 1,\n",
       "         'tenida': 1,\n",
       "         'tenidas': 1,\n",
       "         'tenido': 1,\n",
       "         'tenidos': 1,\n",
       "         'teniendo': 1,\n",
       "         'ten√©is': 1,\n",
       "         'ten√≠a': 1,\n",
       "         'ten√≠ais': 1,\n",
       "         'ten√≠amos': 1,\n",
       "         'ten√≠an': 1,\n",
       "         'ten√≠as': 1,\n",
       "         'tercera': 1,\n",
       "         'ti': 1,\n",
       "         'tiempo': 1,\n",
       "         'tiene': 1,\n",
       "         'tienen': 1,\n",
       "         'tienes': 1,\n",
       "         'toda': 1,\n",
       "         'todas': 1,\n",
       "         'todav√≠a': 1,\n",
       "         'todo': 1,\n",
       "         'todos': 1,\n",
       "         'total': 1,\n",
       "         'trabaja': 1,\n",
       "         'trabajais': 1,\n",
       "         'trabajamos': 1,\n",
       "         'trabajan': 1,\n",
       "         'trabajar': 1,\n",
       "         'trabajas': 1,\n",
       "         'trabajo': 1,\n",
       "         'tras': 1,\n",
       "         'trata': 1,\n",
       "         'trav√©s': 1,\n",
       "         'tres': 1,\n",
       "         'tu': 1,\n",
       "         'tus': 1,\n",
       "         'tuve': 1,\n",
       "         'tuviera': 1,\n",
       "         'tuvierais': 1,\n",
       "         'tuvieran': 1,\n",
       "         'tuvieras': 1,\n",
       "         'tuvieron': 1,\n",
       "         'tuviese': 1,\n",
       "         'tuvieseis': 1,\n",
       "         'tuviesen': 1,\n",
       "         'tuvieses': 1,\n",
       "         'tuvimos': 1,\n",
       "         'tuviste': 1,\n",
       "         'tuvisteis': 1,\n",
       "         'tuvi√©ramos': 1,\n",
       "         'tuvi√©semos': 1,\n",
       "         'tuvo': 1,\n",
       "         'tuya': 1,\n",
       "         'tuyas': 1,\n",
       "         'tuyo': 1,\n",
       "         'tuyos': 1,\n",
       "         't√∫': 1,\n",
       "         'ultimo': 1,\n",
       "         'un': 1,\n",
       "         'una': 1,\n",
       "         'unas': 1,\n",
       "         'uno': 1,\n",
       "         'unos': 1,\n",
       "         'usa': 1,\n",
       "         'usais': 1,\n",
       "         'usamos': 1,\n",
       "         'usan': 1,\n",
       "         'usar': 1,\n",
       "         'usas': 1,\n",
       "         'uso': 1,\n",
       "         'usted': 1,\n",
       "         'va': 1,\n",
       "         'vais': 1,\n",
       "         'valor': 1,\n",
       "         'vamos': 1,\n",
       "         'van': 1,\n",
       "         'varias': 1,\n",
       "         'varios': 1,\n",
       "         'vaya': 1,\n",
       "         'veces': 1,\n",
       "         'ver': 1,\n",
       "         'verdad': 1,\n",
       "         'verdadera': 1,\n",
       "         'verdadero': 1,\n",
       "         'vez': 1,\n",
       "         'vosotras': 1,\n",
       "         'vosotros': 1,\n",
       "         'voy': 1,\n",
       "         'vuestra': 1,\n",
       "         'vuestras': 1,\n",
       "         'vuestro': 1,\n",
       "         'vuestros': 1,\n",
       "         'y': 1,\n",
       "         'ya': 1,\n",
       "         'yo': 1,\n",
       "         '√©l': 1,\n",
       "         '√©ramos': 1,\n",
       "         '√©sta': 1,\n",
       "         '√©stas': 1,\n",
       "         '√©ste': 1,\n",
       "         '√©stos': 1,\n",
       "         '√∫ltima': 1,\n",
       "         '√∫ltimas': 1,\n",
       "         '√∫ltimo': 1,\n",
       "         '√∫ltimos': 1})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "from nltk.stem import SnowballStemmer\n",
    "\n",
    "stemmer = SnowballStemmer('spanish')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_tokenizer(doc, lower=False):\n",
    "    if lower:\n",
    "        tokenized_doc = doc.translate(str.maketrans('','',punctuation)).lower().split()\n",
    "    else:\n",
    "        tokenized_doc = doc.translate(str.maketrans('','',punctuation)).split()\n",
    "    \n",
    "    tokenized_doc = [stemmer.stem(token) for token in tokenized_doc if token.lower() not in stopwords]\n",
    "    return tokenized_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_content = [simple_tokenizer(doc) for doc in content.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ministr',\n",
       " 'cubill',\n",
       " 'extension',\n",
       " 'par',\n",
       " 'docent',\n",
       " 'problem',\n",
       " 'propuest',\n",
       " 'ministr',\n",
       " 'educ',\n",
       " 'marcel',\n",
       " 'cubill',\n",
       " 'refir',\n",
       " 'a',\n",
       " 'votacion',\n",
       " 'realiz',\n",
       " 'interior',\n",
       " 'colegi',\n",
       " 'profesor',\n",
       " 'decid',\n",
       " 'manten',\n",
       " 'par',\n",
       " 'pes',\n",
       " 'llam',\n",
       " 'president',\n",
       " 'gremi',\n",
       " 'mari',\n",
       " 'aguil',\n",
       " 'a',\n",
       " 'repleg',\n",
       " 'entrev',\n",
       " 'program',\n",
       " 'expres',\n",
       " 'bio',\n",
       " 'bio',\n",
       " 'radi',\n",
       " 'ministr',\n",
       " 'valor',\n",
       " 'dich',\n",
       " 'aguil',\n",
       " 'previ',\n",
       " 'a',\n",
       " 'votacion',\n",
       " 'asegur',\n",
       " 'efect',\n",
       " 'posit',\n",
       " 'vuelt',\n",
       " 'clas',\n",
       " 'par',\n",
       " 'profesor',\n",
       " 'extiend',\n",
       " 'seman',\n",
       " '255',\n",
       " 'vot',\n",
       " 'marc',\n",
       " 'diferent',\n",
       " 'sufragi',\n",
       " '95',\n",
       " 'colegi',\n",
       " 'funcion',\n",
       " 'normal',\n",
       " 'baj',\n",
       " '490',\n",
       " 'colegi',\n",
       " 'qued',\n",
       " 'par',\n",
       " 'a',\n",
       " 'ministeri',\n",
       " 'segu',\n",
       " 'negoci',\n",
       " 'ofrec',\n",
       " 'consider',\n",
       " 'mayor',\n",
       " 'vot',\n",
       " 'extend',\n",
       " 'moviliz',\n",
       " 'ministr',\n",
       " 'limit',\n",
       " 'a',\n",
       " 'esper',\n",
       " 'resuelv',\n",
       " 'dialog',\n",
       " 'intern',\n",
       " 'llev',\n",
       " 'a',\n",
       " 'cab',\n",
       " 'interior',\n",
       " 'gremi',\n",
       " 'line',\n",
       " 'sostuv',\n",
       " 'decision',\n",
       " 'president',\n",
       " 'colegi',\n",
       " 'punt',\n",
       " 'negoci',\n",
       " 'llam',\n",
       " 'a',\n",
       " 'acept',\n",
       " 'propuest',\n",
       " 'problem',\n",
       " 'propuest',\n",
       " 'esper',\n",
       " 'opinion',\n",
       " 'interior',\n",
       " 'colegi',\n",
       " 'decant',\n",
       " 'magisteri',\n",
       " 'pid',\n",
       " 'a',\n",
       " 'docent',\n",
       " 'ces',\n",
       " 'par',\n",
       " 'ofert',\n",
       " '45',\n",
       " 'mil',\n",
       " 'trimestral',\n",
       " 'a',\n",
       " 'educ',\n",
       " 'diferencial',\n",
       " 'consult',\n",
       " 'dond',\n",
       " 'problem',\n",
       " 'a',\n",
       " 'parec',\n",
       " 'jef',\n",
       " 'carter',\n",
       " 'reiter',\n",
       " 'dialog',\n",
       " 'intern',\n",
       " 'pendient',\n",
       " 'reconoc',\n",
       " 'pag',\n",
       " 'mencion',\n",
       " 'a',\n",
       " 'educ',\n",
       " 'diferencial',\n",
       " 'parvul',\n",
       " 'cubill',\n",
       " 'adelant',\n",
       " 'gobiern',\n",
       " 'dispuest',\n",
       " 'a',\n",
       " 'discut',\n",
       " 'nuev',\n",
       " 'punt',\n",
       " 'se√±al',\n",
       " 'qued',\n",
       " 'conform',\n",
       " 'plant',\n",
       " 'petitori',\n",
       " 'dia',\n",
       " 'pas',\n",
       " 'entramp',\n",
       " 'demor',\n",
       " 'empez',\n",
       " 'a',\n",
       " 'avanz',\n",
       " 'punt',\n",
       " 'acuerd',\n",
       " 'critic',\n",
       " 'cubill',\n",
       " 'llam',\n",
       " 'direct',\n",
       " 'a',\n",
       " 'colegi',\n",
       " '5',\n",
       " 'sig',\n",
       " 'par',\n",
       " 'a',\n",
       " 'ojal',\n",
       " 'depong',\n",
       " 'aguil',\n",
       " 'alcanc',\n",
       " 'par',\n",
       " 'docent',\n",
       " 'mostr',\n",
       " 'organiz',\n",
       " 'enfrent',\n",
       " 'a',\n",
       " 'gobi',\n",
       " 'empresarial',\n",
       " 'ministeri',\n",
       " 'energ',\n",
       " 'puest',\n",
       " 'plan',\n",
       " 'recuper',\n",
       " 'repar',\n",
       " 'da√±',\n",
       " 'produc',\n",
       " 'a',\n",
       " 'ni√±',\n",
       " 'educ',\n",
       " 'public',\n",
       " 'paraliz',\n",
       " 'punt',\n",
       " 'exig',\n",
       " 'colegi',\n",
       " 'profesor',\n",
       " 'primer',\n",
       " 'seman',\n",
       " 'moviliz',\n",
       " 'ministr',\n",
       " 'particip',\n",
       " 'negoci',\n",
       " 'instanci',\n",
       " 'lider',\n",
       " 'subsecretari',\n",
       " 'line',\n",
       " 'descart',\n",
       " 'mea',\n",
       " 'culp',\n",
       " 'sum',\n",
       " 'a',\n",
       " 'mes',\n",
       " 'recien',\n",
       " 'cuart',\n",
       " 'seman',\n",
       " 'par',\n",
       " 'gobiern',\n",
       " 'actu',\n",
       " 'predisposicion',\n",
       " 'posicion',\n",
       " 'direct',\n",
       " 'colegi',\n",
       " 'profesor',\n",
       " 'habl',\n",
       " 'subsecrterari',\n",
       " 'gobiern',\n",
       " 'actu',\n",
       " 'opinion',\n",
       " 'a',\n",
       " 'problem',\n",
       " 'convers',\n",
       " 'colegi',\n",
       " 'profesor',\n",
       " 'escuch',\n",
       " 'entrev',\n",
       " 'complet']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_content[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T19:41:36.055210Z",
     "start_time": "2019-08-26T19:41:36.051221Z"
    }
   },
   "source": [
    "### Extracci√≥n de Frases\n",
    "\n",
    "Para crear buenas representaciones, es necesario tambien encontrar conjuntos de palabras que por si solas no tengan mayor significado (como `nueva` y `york`), pero que juntas que representen ideas concretas (`nueva york`). \n",
    "\n",
    "Para esto, usaremos el primer conjunto de herramientas de `gensim`: `Phrases` y `Phraser`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:18:18.064454Z",
     "start_time": "2020-05-07T19:18:03.208281Z"
    }
   },
   "outputs": [],
   "source": [
    "#La condici√≥n para que sean considerados es que aparezcan por lo menos 100 veces repetidas.\n",
    "\n",
    "phrases = Phrases(cleaned_content, min_count = 100, progress_per = 5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, usamos `Phraser` para re-tokenizamos el corpus con los bigramas encontrados. Es decir, juntamos los tokens separados que detectamos como frases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colaps': 584,\n",
       " 'segment': 175,\n",
       " 'colaps_segment': 1,\n",
       " 'cas': 17621,\n",
       " 'segment_cas': 1,\n",
       " 'derrumb': 536,\n",
       " 'cas_derrumb': 6,\n",
       " 'valparais': 2017,\n",
       " 'derrumb_valparais': 11,\n",
       " 'notici': 1675,\n",
       " 'valparais_notici': 1,\n",
       " 'desarroll': 4340,\n",
       " 'notici_desarroll': 8,\n",
       " 'recopil': 201,\n",
       " 'desarroll_recopil': 1,\n",
       " 'antecedent': 2320,\n",
       " 'recopil_antecedent': 25,\n",
       " 'antecedent_notici': 1,\n",
       " 'quedat': 12,\n",
       " 'notici_quedat': 1,\n",
       " 'atent': 1598,\n",
       " 'quedat_atent': 1,\n",
       " 'a': 241340,\n",
       " 'atent_a': 215,\n",
       " 'actualiz': 376,\n",
       " 'a_actualiz': 19,\n",
       " 'estructur': 624,\n",
       " 'actualiz_estructur': 1,\n",
       " 'restant': 200,\n",
       " 'estructur_restant': 1,\n",
       " 'restant_cas': 2,\n",
       " 'cay': 724,\n",
       " 'cas_cay': 2,\n",
       " 'cay_valparais': 1,\n",
       " 'valparais_colaps': 1,\n",
       " 'ma√±an': 2853,\n",
       " 'colaps_ma√±an': 2,\n",
       " 'miercol': 5322,\n",
       " 'ma√±an_miercol': 159,\n",
       " 'produj': 1014,\n",
       " 'miercol_produj': 3,\n",
       " 'produj_a': 70,\n",
       " '1015': 13,\n",
       " 'a_1015': 4,\n",
       " 'hor': 8655,\n",
       " '1015_hor': 3,\n",
       " 'esquin': 233,\n",
       " 'hor_esquin': 2,\n",
       " 'aldunat': 19,\n",
       " 'esquin_aldunat': 1,\n",
       " 'huit': 10,\n",
       " 'aldunat_huit': 8,\n",
       " 'mur': 3672,\n",
       " 'huit_mur': 1,\n",
       " 'person': 16681,\n",
       " 'mur_person': 44,\n",
       " 'inform': 10339,\n",
       " 'person_inform': 17,\n",
       " 'prelimin': 403,\n",
       " 'inform_prelimin': 167,\n",
       " 'rescat': 1968,\n",
       " 'prelimin_rescat': 1,\n",
       " 'labor': 1400,\n",
       " 'rescat_labor': 1,\n",
       " 'suspend': 1853,\n",
       " 'labor_suspend': 2,\n",
       " 'peligr': 1892,\n",
       " 'suspend_peligr': 1,\n",
       " 'segu': 3708,\n",
       " 'peligr_segu': 4,\n",
       " 'trabaj': 7554,\n",
       " 'segu_trabaj': 118,\n",
       " 'lesion': 2204,\n",
       " 'trabaj_lesion': 2,\n",
       " 'polic': 6804,\n",
       " 'busc': 6991,\n",
       " 'polic_busc': 33,\n",
       " 'busc_a': 355,\n",
       " 'muj': 5790,\n",
       " 'a_muj': 752,\n",
       " 'acus': 9550,\n",
       " 'muj_acus': 55,\n",
       " 'mat': 1770,\n",
       " 'acus_mat': 59,\n",
       " 'mat_a': 934,\n",
       " 'padr': 3706,\n",
       " 'a_padr': 498,\n",
       " 'discusion': 1129,\n",
       " 'padr_discusion': 2,\n",
       " 'vent': 1802,\n",
       " 'discusion_vent': 1,\n",
       " 'viviend': 2437,\n",
       " 'vent_viviend': 6,\n",
       " 'pudahuel': 140,\n",
       " 'viviend_pudahuel': 3,\n",
       " 'detectiv': 220,\n",
       " 'pudahuel_detectiv': 1,\n",
       " 'detectiv_polic': 19,\n",
       " 'investig': 12695,\n",
       " 'polic_investig': 654,\n",
       " 'realiz': 5745,\n",
       " 'investig_realiz': 82,\n",
       " 'peritaj': 302,\n",
       " 'realiz_peritaj': 52,\n",
       " 'deten': 5620,\n",
       " 'peritaj_deten': 1,\n",
       " 'deten_a': 353,\n",
       " '45': 787,\n",
       " 'muj_45': 11,\n",
       " 'a√±os': 21771,\n",
       " '45_a√±os': 148,\n",
       " 'presunt': 2345,\n",
       " 'a√±os_presunt': 21,\n",
       " 'respons': 4713,\n",
       " 'presunt_respons': 136,\n",
       " 'ataqu': 4166,\n",
       " 'respons_ataqu': 68,\n",
       " 'arma': 1079,\n",
       " 'ataqu_arma': 20,\n",
       " 'cortant': 36,\n",
       " 'arma_cortant': 7,\n",
       " 'cortant_padr': 1,\n",
       " 'caus': 5307,\n",
       " 'padr_caus': 3,\n",
       " 'muert': 8445,\n",
       " 'caus_muert': 361,\n",
       " 'comun': 10804,\n",
       " 'muert_comun': 7,\n",
       " 'comun_pudahuel': 21,\n",
       " 'ocurr': 6019,\n",
       " 'pudahuel_ocurr': 4,\n",
       " 'call': 3640,\n",
       " 'ocurr_call': 32,\n",
       " 'president': 20387,\n",
       " 'call_president': 6,\n",
       " 'trum': 23,\n",
       " 'president_trum': 2,\n",
       " 'interseccion': 324,\n",
       " 'trum_interseccion': 2,\n",
       " 'tenient': 350,\n",
       " 'interseccion_tenient': 2,\n",
       " 'cruz': 1619,\n",
       " 'tenient_cruz': 5,\n",
       " 'acord': 1023,\n",
       " 'cruz_acord': 1,\n",
       " 'acord_a': 133,\n",
       " 'declar': 8661,\n",
       " 'a_declar': 416,\n",
       " 'hij': 5439,\n",
       " 'declar_hij': 14,\n",
       " 'victim': 5716,\n",
       " 'hij_victim': 30,\n",
       " 'herman': 1805,\n",
       " 'victim_herman': 8,\n",
       " 'victimari': 106,\n",
       " 'herman_victimari': 2,\n",
       " 'sostuv': 2507,\n",
       " 'victimari_sostuv': 1,\n",
       " 'enfrent': 3611,\n",
       " 'sostuv_enfrent': 3,\n",
       " 'verbal': 150,\n",
       " 'enfrent_verbal': 7,\n",
       " 'verbal_a': 18,\n",
       " 'intension': 10,\n",
       " 'a_intension': 1,\n",
       " 'hern': 46,\n",
       " 'intension_hern': 1,\n",
       " 'silv': 922,\n",
       " 'hern_silv': 1,\n",
       " 'perez': 920,\n",
       " 'silv_perez': 1,\n",
       " 'vend': 1380,\n",
       " 'perez_vend': 1,\n",
       " 'vend_cas': 8,\n",
       " 'negoci': 4755,\n",
       " 'cas_negoci': 7,\n",
       " 'negoci_caus': 1,\n",
       " 'molesti': 384,\n",
       " 'caus_molesti': 30,\n",
       " 'molesti_hij': 2,\n",
       " 'tani': 37,\n",
       " 'hij_tani': 1,\n",
       " 'tani_silv': 2,\n",
       " 'silv_discusion': 1,\n",
       " 'acud': 1130,\n",
       " 'discusion_acud': 2,\n",
       " 'acud_a': 539,\n",
       " 'cocin': 374,\n",
       " 'a_cocin': 27,\n",
       " 'cocin_viviend': 3,\n",
       " 'volv': 2700,\n",
       " 'viviend_volv': 1,\n",
       " 'cuchill': 358,\n",
       " 'volv_cuchill': 1,\n",
       " 'apu√±al': 417,\n",
       " 'cuchill_apu√±al': 3,\n",
       " 'apu√±al_a': 119,\n",
       " 'primer': 696,\n",
       " 'padr_primer': 1,\n",
       " 'diligent': 766,\n",
       " 'primer_diligent': 33,\n",
       " 'diligent_realiz': 35,\n",
       " 'carabiner': 5079,\n",
       " 'realiz_carabiner': 11,\n",
       " '45¬∫': 7,\n",
       " 'carabiner_45¬∫': 1,\n",
       " 'comis': 697,\n",
       " '45¬∫_comis': 1,\n",
       " 'tom': 6868,\n",
       " 'comis_tom': 1,\n",
       " 'tom_declar': 100,\n",
       " 'unic': 2866,\n",
       " 'declar_unic': 6,\n",
       " 'testig': 1280,\n",
       " 'unic_testig': 4,\n",
       " 'crim': 1229,\n",
       " 'testig_crim': 2,\n",
       " 'interior': 3664,\n",
       " 'crim_interior': 2,\n",
       " 'interior_viviend': 66,\n",
       " 'capitan': 679,\n",
       " 'viviend_capitan': 1,\n",
       " 'carl': 2272,\n",
       " 'capitan_carl': 12,\n",
       " 'lag': 1972,\n",
       " 'carl_lag': 3,\n",
       " 'principal': 3793,\n",
       " 'lag_principal': 2,\n",
       " 'hipotesis': 250,\n",
       " 'principal_hipotesis': 17,\n",
       " 'apunt': 2217,\n",
       " 'hipotesis_apunt': 6,\n",
       " 'apunt_a': 1007,\n",
       " 'a_discusion': 64,\n",
       " 'diner': 2661,\n",
       " 'discusion_diner': 1,\n",
       " 'diner_vent': 7,\n",
       " 'inmuebl': 485,\n",
       " 'vent_inmuebl': 6,\n",
       " 'inmuebl_detectiv': 1,\n",
       " 'brig': 868,\n",
       " 'detectiv_brig': 40,\n",
       " 'homicidi': 1712,\n",
       " 'brig_homicidi': 350,\n",
       " 'pdi': 1322,\n",
       " 'homicidi_pdi': 131,\n",
       " 'pdi_realiz': 16,\n",
       " 'corrobor': 171,\n",
       " 'peritaj_corrobor': 2,\n",
       " 'corrobor_victim': 2,\n",
       " 'victim_mur': 28,\n",
       " 'mur_apu√±al': 8,\n",
       " 'ocasion': 1541,\n",
       " 'apu√±al_ocasion': 10,\n",
       " 'ocasion_a': 90,\n",
       " 'altur': 975,\n",
       " 'a_altur': 505,\n",
       " 'torax': 64,\n",
       " 'altur_torax': 10,\n",
       " 'subcomisari': 114,\n",
       " 'torax_subcomisari': 1,\n",
       " 'cristian': 863,\n",
       " 'subcomisari_cristian': 3,\n",
       " 'tur': 11,\n",
       " 'cristian_tur': 2,\n",
       " 'tur_presunt': 1,\n",
       " 'identific': 2109,\n",
       " 'respons_identific': 11,\n",
       " 'razon': 2155,\n",
       " 'identific_razon': 4,\n",
       " 'razon_diligent': 1,\n",
       " 'diligent_polic': 8,\n",
       " 'enfoc': 371,\n",
       " 'polic_enfoc': 3,\n",
       " 'parader': 439,\n",
       " 'enfoc_parader': 1,\n",
       " 'parader_deten': 3,\n",
       " 'a_tani': 3,\n",
       " 'herrer': 143,\n",
       " 'silv_herrer': 1,\n",
       " 'herrer_45': 1,\n",
       " 'a√±os_acord': 2,\n",
       " 'expres': 1849,\n",
       " 'a_expres': 42,\n",
       " 'familiar': 1376,\n",
       " 'expres_familiar': 1,\n",
       " 'viv': 4990,\n",
       " 'familiar_viv': 4,\n",
       " 'situacion': 7561,\n",
       " 'viv_situacion': 65,\n",
       " 'situacion_call': 211,\n",
       " 'call_busc': 3,\n",
       " 'delit': 3706,\n",
       " 'busc_delit': 3,\n",
       " 'parricidi': 71,\n",
       " 'delit_parricidi': 32,\n",
       " 'articul': 2502,\n",
       " 'parricidi_articul': 1,\n",
       " 'describ': 1152,\n",
       " 'articul_describ': 530,\n",
       " 'proces': 6189,\n",
       " 'describ_proces': 527,\n",
       " 'judicial': 2900,\n",
       " 'proces_judicial': 698,\n",
       " 'curs': 1583,\n",
       " 'judicial_curs': 536,\n",
       " 'posibil': 2456,\n",
       " 'curs_posibil': 526,\n",
       " 'carg': 6419,\n",
       " 'posibil_carg': 526,\n",
       " 'desestim': 826,\n",
       " 'carg_desestim': 527,\n",
       " 'finaliz': 1233,\n",
       " 'desestim_finaliz': 526,\n",
       " 'finaliz_investig': 535,\n",
       " 'consider': 5351,\n",
       " 'investig_consider': 542,\n",
       " 'imput': 2615,\n",
       " 'consider_imput': 530,\n",
       " 'culpabl': 1372,\n",
       " 'imput_culpabl': 528,\n",
       " 'justici': 5313,\n",
       " 'culpabl_justici': 529,\n",
       " 'dict': 1095,\n",
       " 'justici_dict': 529,\n",
       " 'sentenci': 1893,\n",
       " 'dict_sentenci': 568,\n",
       " 'sentenci_articul': 526,\n",
       " '04': 587,\n",
       " 'articul_04': 526,\n",
       " 'codig': 965,\n",
       " '04_codig': 526,\n",
       " 'procesal': 630,\n",
       " 'codig_procesal': 539,\n",
       " 'penal': 2203,\n",
       " 'procesal_penal': 556,\n",
       " 'lice': 493,\n",
       " 'deten_lice': 1,\n",
       " 'aplic': 2643,\n",
       " 'lice_aplic': 14,\n",
       " 'protagon': 427,\n",
       " 'aplic_protagon': 1,\n",
       " 'incendi': 2165,\n",
       " 'protagon_incendi': 1,\n",
       " 'ba√±': 563,\n",
       " 'incendi_ba√±': 1,\n",
       " 'quem': 1027,\n",
       " 'ba√±_quem': 2,\n",
       " 'capuch': 27,\n",
       " 'quem_capuch': 1,\n",
       " 'overol': 28,\n",
       " 'capuch_overol': 1,\n",
       " 'overol_deten': 1,\n",
       " 'sald': 468,\n",
       " 'deten_sald': 4,\n",
       " 'seri': 2810,\n",
       " 'sald_seri': 1,\n",
       " 'incident': 1672,\n",
       " 'seri_incident': 26,\n",
       " 'incident_ocurr': 87,\n",
       " 'ocurr_ma√±an': 36,\n",
       " 'ma√±an_lice': 1,\n",
       " 'santiag': 3179,\n",
       " 'aplic_santiag': 2,\n",
       " 'inclu': 3828,\n",
       " 'santiag_inclu': 7,\n",
       " 'fogat': 36,\n",
       " 'inclu_fogat': 1,\n",
       " 'fogat_interior': 2,\n",
       " 'interior_ba√±': 4,\n",
       " 'ba√±_a': 14,\n",
       " '800': 361,\n",
       " 'a_800': 60,\n",
       " '800_ma√±an': 2,\n",
       " 'grup': 6321,\n",
       " 'ma√±an_grup': 10,\n",
       " 'encapuch': 478,\n",
       " 'grup_encapuch': 42,\n",
       " 'disturbi': 308,\n",
       " 'encapuch_disturbi': 1,\n",
       " 'disturbi_interseccion': 1,\n",
       " 'cumming': 13,\n",
       " 'interseccion_cumming': 2,\n",
       " 'erasm': 8,\n",
       " 'cumming_erasm': 1,\n",
       " 'escal': 886,\n",
       " 'erasm_escal': 1,\n",
       " 'escal_inclu': 3,\n",
       " 'barric': 221,\n",
       " 'inclu_barric': 2,\n",
       " 'evacu': 1075,\n",
       " 'barric_evacu': 1,\n",
       " 'espontane': 65,\n",
       " 'evacu_espontane': 3,\n",
       " 'institut': 1830,\n",
       " 'espontane_institut': 2,\n",
       " 'nacional': 10685,\n",
       " 'institut_nacional': 822,\n",
       " 'efect': 3858,\n",
       " 'nacional_efect': 7,\n",
       " 'bomb': 977,\n",
       " 'efect_bomb': 4,\n",
       " 'lacrimogen': 273,\n",
       " 'bomb_lacrimogen': 67,\n",
       " 'lacrimogen_carabiner': 7,\n",
       " 'carabiner_a': 135,\n",
       " 'lleg': 8504,\n",
       " 'a_lleg': 389,\n",
       " 'fuerz': 4530,\n",
       " 'lleg_fuerz': 8,\n",
       " 'especial': 4011,\n",
       " 'fuerz_especial': 280,\n",
       " 'especial_carabiner': 100,\n",
       " 'manifest': 4449,\n",
       " 'carabiner_manifest': 12,\n",
       " 'lanz': 2585,\n",
       " 'manifest_lanz': 27,\n",
       " 'lanz_bomb': 102,\n",
       " 'molotov': 222,\n",
       " 'bomb_molotov': 153,\n",
       " 'personal': 4093,\n",
       " 'molotov_personal': 3,\n",
       " 'huir': 354,\n",
       " 'personal_huir': 1,\n",
       " 'huir_interior': 1,\n",
       " 'establec': 4350,\n",
       " 'interior_establec': 44,\n",
       " 'report': 2143,\n",
       " 'establec_report': 1,\n",
       " 'gonzal': 462,\n",
       " 'report_gonzal': 1,\n",
       " 'urbin': 22,\n",
       " 'gonzal_urbin': 4,\n",
       " 'uniform': 968,\n",
       " 'urbin_uniform': 1,\n",
       " 'uniform_encapuch': 1,\n",
       " 'inici': 6343,\n",
       " 'encapuch_inici': 2,\n",
       " 'inici_fogat': 3,\n",
       " 'fogat_ba√±': 1,\n",
       " 'quem_overol': 2,\n",
       " 'overol_capuch': 1,\n",
       " 'mochil': 240,\n",
       " 'capuch_mochil': 1,\n",
       " 'utiliz': 2866,\n",
       " 'mochil_utiliz': 2,\n",
       " 'utiliz_personal': 5,\n",
       " 'policial': 2457,\n",
       " 'personal_policial': 233,\n",
       " 'ingres': 4549,\n",
       " 'policial_ingres': 11,\n",
       " 'recint': 1850,\n",
       " 'ingres_recint': 42,\n",
       " 'logr': 4949,\n",
       " 'recint_logr': 7,\n",
       " 'logr_deten': 64,\n",
       " 'joven': 1567,\n",
       " 'a_joven': 242,\n",
       " 'port': 895,\n",
       " 'joven_port': 2,\n",
       " 'port_bomb': 11,\n",
       " 'incendiari': 282,\n",
       " 'bomb_incendiari': 15,\n",
       " 'incendiari_urbin': 1,\n",
       " 'autor': 8979,\n",
       " 'urbin_autor': 1,\n",
       " 'autor_lice': 1,\n",
       " 'indic': 4813,\n",
       " 'lice_indic': 1,\n",
       " 'indic_protagon': 2,\n",
       " 'hech': 3093,\n",
       " 'protagon_hech': 2,\n",
       " 'estudi': 6680,\n",
       " 'hech_estudi': 7,\n",
       " 'estudi_recint': 9,\n",
       " 'recint_investig': 5,\n",
       " 'investig_a': 395,\n",
       " 'colegi': 2260,\n",
       " 'a_colegi': 90,\n",
       " 'pertenec': 719,\n",
       " 'colegi_pertenec': 1,\n",
       " 'aprehend': 76,\n",
       " 'pertenec_aprehend': 1,\n",
       " 'alumn': 1127,\n",
       " 'aprehend_alumn': 2,\n",
       " 'alumn_colegi': 28,\n",
       " 'denunci': 6315,\n",
       " 'colegi_denunci': 1,\n",
       " 'indiscrimin': 53,\n",
       " 'denunci_indiscrimin': 2,\n",
       " 'gas': 1247,\n",
       " 'indiscrimin_gas': 1,\n",
       " 'gas_lacrimogen': 190,\n",
       " 'simil': 748,\n",
       " 'carabiner_simil': 1,\n",
       " 'simil_a': 194,\n",
       " 'a_ocurr': 253,\n",
       " 'ocurr_institut': 6,\n",
       " 'rodrig': 912,\n",
       " 'nacional_rodrig': 22,\n",
       " 'pin': 323,\n",
       " 'rodrig_pin': 94,\n",
       " 'rbb': 1103,\n",
       " 'pin_rbb': 94,\n",
       " 'bomber': 1839,\n",
       " 'rbb_bomber': 2,\n",
       " 'control': 4263,\n",
       " 'bomber_control': 24,\n",
       " 'control_situacion': 64,\n",
       " 'clas': 1566,\n",
       " 'situacion_clas': 4,\n",
       " 'clas_suspend': 27,\n",
       " 'apoy': 5496,\n",
       " 'transversal': 179,\n",
       " 'apoy_transversal': 18,\n",
       " 'sen': 1990,\n",
       " 'transversal_sen': 2,\n",
       " 'aprueb': 550,\n",
       " 'sen_aprueb': 33,\n",
       " 'general': 5267,\n",
       " 'aprueb_general': 12,\n",
       " 'proyect': 7306,\n",
       " 'general_proyect': 39,\n",
       " 'ley': 6092,\n",
       " 'proyect_ley': 898,\n",
       " 'migracion': 923,\n",
       " 'ley_migracion': 26,\n",
       " 'gobiern': 18603,\n",
       " 'migracion_gobiern': 6,\n",
       " 'sal': 7598,\n",
       " 'gobiern_sal': 21,\n",
       " 'sal_sen': 62,\n",
       " 'aprob': 3092,\n",
       " 'sen_aprob': 50,\n",
       " 'aprob_general': 45,\n",
       " 'present': 8889,\n",
       " 'migracion_present': 2,\n",
       " 'present_gobiern': 57,\n",
       " 'gobiern_inici': 33,\n",
       " 'entrar': 869,\n",
       " 'inici_entrar': 3,\n",
       " 'entrar_a': 196,\n",
       " 'fas': 374,\n",
       " 'a_fas': 39,\n",
       " 'fas_indic': 2,\n",
       " 'indic_discusion': 4,\n",
       " 'particul': 1287,\n",
       " 'discusion_particul': 21,\n",
       " 'particul_inici': 4,\n",
       " 'inici_busc': 83,\n",
       " 'modific': 629,\n",
       " 'busc_modific': 40,\n",
       " 'polit': 9539,\n",
       " 'modific_polit': 8,\n",
       " 'migratori': 1019,\n",
       " 'polit_migratori': 183,\n",
       " 'pais': 20632,\n",
       " 'migratori_pais': 19,\n",
       " 'reform': 2601,\n",
       " 'pais_reform': 13,\n",
       " 'a√±o': 10074,\n",
       " 'reform_a√±o': 6,\n",
       " '1975': 62,\n",
       " 'a√±o_1975': 2,\n",
       " 'votacion': 1368,\n",
       " '1975_votacion': 1,\n",
       " 'resolv': 1117,\n",
       " 'votacion_resolv': 3,\n",
       " '41': 382,\n",
       " 'resolv_41': 1,\n",
       " 'vot': 4374,\n",
       " '41_vot': 7,\n",
       " 'vot_a': 564,\n",
       " 'favor': 2277,\n",
       " 'a_favor': 1294,\n",
       " '0': 187,\n",
       " 'favor_0': 2,\n",
       " '0_0': 24,\n",
       " 'abstencion': 255,\n",
       " '0_abstencion': 2,\n",
       " 'abrir': 677,\n",
       " 'abstencion_abrir': 1,\n",
       " 'period': 5677,\n",
       " 'abrir_period': 2,\n",
       " 'period_ingres': 5,\n",
       " 'ingres_indic': 24,\n",
       " 'intervencion': 1250,\n",
       " 'indic_intervencion': 1,\n",
       " 'senador': 2323,\n",
       " 'intervencion_senador': 5,\n",
       " 'part': 7815,\n",
       " 'senador_part': 48,\n",
       " 'social': 7569,\n",
       " 'part_social': 467,\n",
       " 'jos': 2059,\n",
       " 'social_jos': 16,\n",
       " 'miguel': 1053,\n",
       " 'jos_miguel': 121,\n",
       " 'insulz': 68,\n",
       " 'miguel_insulz': 46,\n",
       " 'insulz_apoy': 1,\n",
       " 'med': 6662,\n",
       " 'apoy_med': 20,\n",
       " 'med_tom': 158,\n",
       " 'tom_gobiern': 26,\n",
       " 'torn': 1230,\n",
       " 'gobiern_torn': 10,\n",
       " 'fluj': 467,\n",
       " 'torn_fluj': 1,\n",
       " 'fluj_migratori': 71,\n",
       " 'produc': 1651,\n",
       " 'migratori_produc': 3,\n",
       " 'crisis': 4356,\n",
       " 'produc_crisis': 3,\n",
       " 'crisis_polit': 315,\n",
       " 'polit_social': 120,\n",
       " 'venezuel': 7076,\n",
       " 'social_venezuel': 19,\n",
       " 'acuerd': 11685,\n",
       " 'venezuel_acuerd': 12,\n",
       " 'fij': 1076,\n",
       " 'acuerd_fij': 8,\n",
       " 'vis': 610,\n",
       " 'fij_vis': 1,\n",
       " 'vis_acuerd': 1,\n",
       " 'decision': 4331,\n",
       " 'acuerd_decision': 9,\n",
       " 'decision_tom': 223,\n",
       " 'consul': 635,\n",
       " 'tom_consul': 1,\n",
       " 'tacn': 86,\n",
       " 'consul_tacn': 10,\n",
       " 'lim': 949,\n",
       " 'tacn_lim': 2,\n",
       " 'lim_sostuv': 2,\n",
       " 'sostuv_senador': 2,\n",
       " 'renov': 972,\n",
       " 'senador_renov': 39,\n",
       " 'renov_nacional': 325,\n",
       " 'francisc': 1738,\n",
       " 'nacional_francisc': 27,\n",
       " 'chahuan': 92,\n",
       " 'francisc_chahuan': 66,\n",
       " 'calific': 2080,\n",
       " 'chahuan_calific': 1,\n",
       " 'legisl': 2941,\n",
       " 'calific_legisl': 2,\n",
       " 'modern': 330,\n",
       " 'legisl_modern': 5,\n",
       " 'asegur': 4650,\n",
       " 'modern_asegur': 1,\n",
       " 'equilibr': 93,\n",
       " 'asegur_equilibr': 1,\n",
       " 'derech': 7616,\n",
       " 'equilibr_derech': 2,\n",
       " 'migrant': 2558,\n",
       " 'derech_migrant': 18,\n",
       " 'migrant_pais': 26,\n",
       " 'regul': 1271,\n",
       " 'pais_regul': 9,\n",
       " 'regul_migracion': 3,\n",
       " 'migracion_inici': 2,\n",
       " 'continu': 4406,\n",
       " 'inici_continu': 4,\n",
       " 'tramit': 1335,\n",
       " 'continu_tramit': 14,\n",
       " 'form': 6958,\n",
       " 'tramit_form': 7,\n",
       " 'form_particul': 18,\n",
       " 'particul_sen': 1,\n",
       " 'carabiner_ma√±an': 5,\n",
       " 'produj_evacu': 1,\n",
       " 'espontane_comun': 1,\n",
       " 'educ': 3447,\n",
       " 'comun_educ': 55,\n",
       " 'educ_institut': 6,\n",
       " 'efect_gas': 2,\n",
       " 'carabiner_interior': 11,\n",
       " 'accion': 4088,\n",
       " 'establec_accion': 8,\n",
       " 'accion_policial': 26,\n",
       " 'respuest': 2347,\n",
       " 'policial_respuest': 1,\n",
       " 'respuest_a': 481,\n",
       " 'a_accion': 201,\n",
       " 'accion_grup': 16,\n",
       " 'menor': 4738,\n",
       " 'grup_menor': 10,\n",
       " 'menor_encapuch': 2,\n",
       " 'encapuch_enfrent': 10,\n",
       " 'enfrent_carabiner': 19,\n",
       " 'techumbr': 74,\n",
       " 'carabiner_techumbr': 1,\n",
       " 'profesor': 2443,\n",
       " 'techumbr_profesor': 1,\n",
       " 'claudi': 704,\n",
       " 'profesor_claudi': 2,\n",
       " 'segovi': 29,\n",
       " 'claudi_segovi': 3,\n",
       " 'segovi_sal': 1,\n",
       " 'sal_recint': 17,\n",
       " 'recint_call': 3,\n",
       " 'alons': 88,\n",
       " 'call_alons': 2,\n",
       " 'ovall': 128,\n",
       " 'alons_ovall': 9,\n",
       " 'ovall_espontane': 1,\n",
       " 'orden': 4527,\n",
       " 'espontane_orden': 2,\n",
       " 'rector': 324,\n",
       " 'orden_rector': 1,\n",
       " 'vieron': 459,\n",
       " 'rector_vieron': 1,\n",
       " 'afect': 6231,\n",
       " 'vieron_afect': 93,\n",
       " 'afect_gas': 8,\n",
       " 'gas_policial': 1,\n",
       " 'policial_acus': 7,\n",
       " 'actu': 1592,\n",
       " 'acus_actu': 22,\n",
       " 'protocol': 788,\n",
       " 'actu_protocol': 5,\n",
       " 'protocol_fuerz': 3,\n",
       " 'advirt': 2010,\n",
       " 'especial_advirt': 1,\n",
       " 'trat': 4361,\n",
       " 'advirt_trat': 6,\n",
       " 'trat_situacion': 20,\n",
       " 'amerit': 81,\n",
       " 'situacion_amerit': 7,\n",
       " 'sumari': 694,\n",
       " 'amerit_sumari': 1,\n",
       " 'intern': 2230,\n",
       " 'sumari_intern': 40,\n",
       " 'numer': 3346,\n",
       " 'intern_numer': 2,\n",
       " 'numer_estudi': 15,\n",
       " 'encontr': 5960,\n",
       " 'estudi_encontr': 18,\n",
       " 'encontr_clas': 7,\n",
       " 'clas_vieron': 1,\n",
       " 'afect_situacion': 33,\n",
       " 'situacion_produj': 13,\n",
       " 'produj_estudi': 2,\n",
       " 'encar': 79,\n",
       " 'estudi_encar': 1,\n",
       " 'encar_a': 19,\n",
       " 'a_carabiner': 474,\n",
       " 'a_sal': 585,\n",
       " 'sal_establec': 13,\n",
       " 'establec_situacion': 10,\n",
       " 'deriv': 1168,\n",
       " 'situacion_deriv': 10,\n",
       " 'empujon': 33,\n",
       " 'deriv_empujon': 1,\n",
       " 'empujon_grup': 1,\n",
       " 'product': 4469,\n",
       " 'grup_product': 4,\n",
       " 'product_person': 6,\n",
       " 'result': 5882,\n",
       " 'person_result': 265,\n",
       " 'result_deten': 14,\n",
       " 'alcald': 2708,\n",
       " 'sharp': 135,\n",
       " 'alcald_sharp': 15,\n",
       " 'lament': 1797,\n",
       " 'sharp_lament': 1,\n",
       " 'mortal': 439,\n",
       " 'lament_mortal': 1,\n",
       " 'mortal_derrumb': 1,\n",
       " 'afirm': 2409,\n",
       " 'derrumb_afirm': 1,\n",
       " 'afirm_valparais': 2,\n",
       " 'ciud': 5032,\n",
       " 'valparais_ciud': 9,\n",
       " 'ciud_viv': 8,\n",
       " 'riesg': 2728,\n",
       " 'viv_riesg': 1,\n",
       " 'riesg_alcald': 1,\n",
       " 'alcald_valparais': 29,\n",
       " 'jorg': 1647,\n",
       " 'valparais_jorg': 52,\n",
       " 'jorg_sharp': 74,\n",
       " 'refir': 1250,\n",
       " 'sharp_refir': 4,\n",
       " 'refir_ma√±an': 5,\n",
       " 'miercol_mortal': 1,\n",
       " 'derrumbr': 1,\n",
       " 'mortal_derrumbr': 1,\n",
       " 'registr': 6542,\n",
       " 'derrumbr_registr': 1,\n",
       " 'anoch': 97,\n",
       " 'registr_anoch': 1,\n",
       " 'anoch_aldunat': 1,\n",
       " 'cerr': 2943,\n",
       " 'huit_cerr': 1,\n",
       " 'bellav': 53,\n",
       " 'cerr_bellav': 1,\n",
       " 'extend': 1115,\n",
       " 'bellav_extend': 1,\n",
       " 'condolent': 154,\n",
       " 'extend_condolent': 3,\n",
       " 'condolent_familiar': 1,\n",
       " 'familiar_victim': 106,\n",
       " 'victim_asegur': 8,\n",
       " 'asegur_ciud': 2,\n",
       " 'conmocion': 267,\n",
       " 'ciud_conmocion': 1,\n",
       " 'conmocion_ocurr': 1,\n",
       " 'ocurr_derrumb': 6,\n",
       " 'derrumb_viviend': 26,\n",
       " 'viviend_cerr': 14,\n",
       " 'cerr_valparais': 25,\n",
       " 'valparais_rescat': 7,\n",
       " '2': 2612,\n",
       " 'rescat_2': 8,\n",
       " 'cuerp': 3012,\n",
       " '2_cuerp': 9,\n",
       " '4': 2661,\n",
       " 'cuerp_4': 7,\n",
       " '4_victim': 9,\n",
       " 'fatal': 589,\n",
       " 'victim_fatal': 230,\n",
       " 'fatal_valparais': 1,\n",
       " 'usted': 374,\n",
       " 'valparais_usted': 1,\n",
       " 'dolor': 973,\n",
       " 'usted_dolor': 1,\n",
       " 'dia': 5959,\n",
       " 'dolor_dia': 1,\n",
       " 'compart': 1868,\n",
       " 'dia_compart': 1,\n",
       " 'compart_ciud': 3,\n",
       " 'ciud_lament': 2,\n",
       " 'lament_valparais': 1,\n",
       " 'permanent': 946,\n",
       " 'viv_permanent': 3,\n",
       " 'permanent_riesg': 4,\n",
       " 'riesg_ciud': 2,\n",
       " 'expuest': 415,\n",
       " 'ciud_expuest': 1,\n",
       " 'expuest_a': 133,\n",
       " 'a_situacion': 438,\n",
       " 'usual': 100,\n",
       " 'situacion_usual': 1,\n",
       " 'gust': 947,\n",
       " 'usual_gust': 1,\n",
       " 'sent': 3346,\n",
       " 'gust_sent': 1,\n",
       " 'profundiz': 279,\n",
       " 'sent_profundiz': 7,\n",
       " 'tip': 3493,\n",
       " 'profundiz_tip': 1,\n",
       " 'desliz': 240,\n",
       " 'tip_desliz': 1,\n",
       " 'evident': 1196,\n",
       " 'desliz_evident': 1,\n",
       " 'magnitud': 649,\n",
       " 'evident_magnitud': 7,\n",
       " 'magnitud_produc': 2,\n",
       " 'produc_ciud': 2,\n",
       " 'ciud_alcald': 6,\n",
       " 'esper': 8092,\n",
       " 'alcald_esper': 3,\n",
       " 'esper_trabaj': 9,\n",
       " 'pued': 1782,\n",
       " 'trabaj_pued': 6,\n",
       " 'proporcion': 540,\n",
       " 'pued_proporcion': 1,\n",
       " 'mayor': 4884,\n",
       " 'proporcion_mayor': 5,\n",
       " 'certez': 248,\n",
       " 'mayor_certez': 3,\n",
       " 'asimim': 1,\n",
       " 'certez_asimim': 1,\n",
       " 'asimim_viviend': 1,\n",
       " 'catastr': 108,\n",
       " 'viviend_catastr': 1,\n",
       " 'catastr_advirt': 1,\n",
       " 'prefier': 258,\n",
       " 'advirt_prefier': 1,\n",
       " 'prudent': 99,\n",
       " 'prefier_prudent': 1,\n",
       " 'prudent_investig': 1,\n",
       " 'investig_curs': 56,\n",
       " 'fiscal': 8908,\n",
       " 'curs_fiscal': 3,\n",
       " 'fiscal_a': 206,\n",
       " 'colabor': 1228,\n",
       " 'a_colabor': 102,\n",
       " 'esclarec': 351,\n",
       " 'colabor_esclarec': 3,\n",
       " 'esclarec_razon': 2,\n",
       " 'jov': 4250,\n",
       " 'jov_result': 27,\n",
       " 'grav': 2643,\n",
       " 'result_grav': 13,\n",
       " 'grav_apu√±al': 4,\n",
       " 'event': 1298,\n",
       " 'apu√±al_event': 1,\n",
       " 'benef': 30,\n",
       " 'event_benef': 5,\n",
       " 'escuel': 1491,\n",
       " 'benef_escuel': 2,\n",
       " 'chonchi': 43,\n",
       " 'escuel_chonchi': 1,\n",
       " 'chonchi_jov': 1,\n",
       " 'jov_grav': 2,\n",
       " 'medi': 11323,\n",
       " 'apu√±al_medi': 1,\n",
       " 'fiest': 721,\n",
       " 'medi_fiest': 7,\n",
       " 'fiest_chonchi': 1,\n",
       " 'provinci': 1980,\n",
       " 'chonchi_provinci': 3,\n",
       " 'chilo': 290,\n",
       " 'provinci_chilo': 41,\n",
       " 'chilo_intern': 1,\n",
       " 'hospital': 3518,\n",
       " 'intern_hospital': 107,\n",
       " 'august': 388,\n",
       " 'hospital_august': 6,\n",
       " 'riffart': 6,\n",
       " 'august_riffart': 6,\n",
       " 'castr': 696,\n",
       " 'riffart_castr': 4,\n",
       " 'permanec': 1363,\n",
       " 'castr_permanec': 2,\n",
       " 'permanec_victim': 3,\n",
       " 'agresion': 1060,\n",
       " 'victim_agresion': 35,\n",
       " 'sufr': 2913,\n",
       " 'agresion_sufr': 32,\n",
       " 'sufr_arma': 1,\n",
       " 'cortopunz': 112,\n",
       " 'arma_cortopunz': 41,\n",
       " 'sector': 5870,\n",
       " 'cortopunz_sector': 2,\n",
       " 'petan': 1,\n",
       " 'sector_petan': 1,\n",
       " 'petan_produj': 1,\n",
       " 'produj_medi': 11,\n",
       " 'medi_event': 4,\n",
       " 'escuel_miguel': 3,\n",
       " 'aguil': 423,\n",
       " 'miguel_aguil': 8,\n",
       " 'aguil_carabiner': 1,\n",
       " 'detall': 4957,\n",
       " 'carabiner_detall': 19,\n",
       " 'agresor': 362,\n",
       " 'detall_agresor': 3,\n",
       " '20': 3264,\n",
       " 'agresor_20': 1,\n",
       " '20_a√±os': 682,\n",
       " 'a√±os_deten': 178,\n",
       " 'funcionari': 3785,\n",
       " 'deten_funcionari': 13,\n",
       " 'funcionari_policial': 105,\n",
       " 'detencion': 2410,\n",
       " 'policial_detencion': 11,\n",
       " 'detencion_agresor': 4,\n",
       " 'ampli': 1904,\n",
       " 'agresor_ampli': 1,\n",
       " 'juev': 5457,\n",
       " 'ampli_juev': 8,\n",
       " 'juev_a': 378,\n",
       " 'a_fiscal': 541,\n",
       " 'fiscal_esper': 18,\n",
       " 'esper_inform': 59,\n",
       " 'medic': 3730,\n",
       " 'inform_medic': 28,\n",
       " 'medic_lesion': 5,\n",
       " 'lesion_present': 10,\n",
       " 'present_victim': 12,\n",
       " 'prision': 2950,\n",
       " 'prevent': 1795,\n",
       " 'prision_prevent': 1174,\n",
       " 'qued': 4620,\n",
       " 'prevent_qued': 51,\n",
       " 'qued_acus': 15,\n",
       " 'viol': 1160,\n",
       " 'acus_viol': 87,\n",
       " 'dej': 6638,\n",
       " 'viol_dej': 2,\n",
       " 'embaraz': 779,\n",
       " 'dej_embaraz': 1,\n",
       " 'embaraz_a': 52,\n",
       " 'a_herman': 279,\n",
       " 'deficient': 200,\n",
       " 'herman_deficient': 1,\n",
       " 'mental': 444,\n",
       " 'deficient_mental': 2,\n",
       " 'mental_cas': 2,\n",
       " 'criminal': 1093,\n",
       " 'cas_criminal': 6,\n",
       " 'criminal_investig': 4,\n",
       " 'desformaliz': 17,\n",
       " 'investig_desformaliz': 14,\n",
       " 'desformaliz_a√±o': 1,\n",
       " 'a√±o_pdi': 3,\n",
       " ...}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "phrases.vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram = Phraser(phrases)\n",
    "sentences = bigram[cleaned_content]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['ministr',\n",
       " 'viviend',\n",
       " 'enfrent',\n",
       " 'polem',\n",
       " 'casit',\n",
       " 'call',\n",
       " 'oficin',\n",
       " 'ministr',\n",
       " 'viviend_urban',\n",
       " 'cristi',\n",
       " 'monckeberg',\n",
       " 'refir',\n",
       " 'a',\n",
       " 'polem',\n",
       " 'reflexion',\n",
       " 'integr',\n",
       " 'social',\n",
       " 'deficit',\n",
       " 'habitacional',\n",
       " 'pais',\n",
       " 'comision',\n",
       " 'viviend_urban',\n",
       " 'sen',\n",
       " 'mayor',\n",
       " 'propietari',\n",
       " 'patrimoni',\n",
       " 'casit',\n",
       " 'departament',\n",
       " 'radic',\n",
       " 'patrimoni',\n",
       " 'chilen',\n",
       " 'episodi',\n",
       " 'entrev',\n",
       " 'program',\n",
       " 'peor',\n",
       " 'juli',\n",
       " 'ces',\n",
       " 'rodriguez',\n",
       " 'remarc',\n",
       " 'dich',\n",
       " 'intervencion',\n",
       " 'larg',\n",
       " '14',\n",
       " 'agost',\n",
       " 'siqu',\n",
       " 'sac',\n",
       " 'context',\n",
       " 'escuch',\n",
       " 'grabacion',\n",
       " 'efect',\n",
       " 'suen',\n",
       " 'horror',\n",
       " 'explic',\n",
       " 'larg',\n",
       " 'tem',\n",
       " 'trat',\n",
       " 'sen',\n",
       " 'a',\n",
       " 'senador',\n",
       " 'llam_atencion',\n",
       " 'ministr',\n",
       " 'viviend',\n",
       " 'mayor',\n",
       " 'propietari',\n",
       " 'casit',\n",
       " 'departament',\n",
       " 'pas',\n",
       " 'instanci',\n",
       " 'analiz',\n",
       " 'chilen',\n",
       " 'cultural',\n",
       " 'propietari',\n",
       " 'aspir',\n",
       " 'a',\n",
       " 'propietari',\n",
       " 'algui',\n",
       " 'sint',\n",
       " 'ofend',\n",
       " 'pid',\n",
       " 'disculp',\n",
       " 'cas',\n",
       " '¬øcuant',\n",
       " 'person',\n",
       " 'viviend',\n",
       " 'chil',\n",
       " 'refir',\n",
       " 'a',\n",
       " 'cifr',\n",
       " 'uso',\n",
       " 'defend',\n",
       " 'admit',\n",
       " '604',\n",
       " 'hogar',\n",
       " 'chilen',\n",
       " '35',\n",
       " 'millon_habit',\n",
       " 'viviend',\n",
       " 'acuerd',\n",
       " 'a',\n",
       " 'encuest',\n",
       " 'cas',\n",
       " '2017',\n",
       " 'consider',\n",
       " 'pag',\n",
       " 'propied',\n",
       " 'credit',\n",
       " 'hipotecari',\n",
       " '‚Äì',\n",
       " '¬øy',\n",
       " 'cas',\n",
       " '‚Äì',\n",
       " 'pront',\n",
       " 'habl',\n",
       " 'departament',\n",
       " 'estacion',\n",
       " 'bodeg',\n",
       " 'cas',\n",
       " 'banc',\n",
       " 'suert',\n",
       " 'due√±',\n",
       " 'ba√±',\n",
       " 'deb',\n",
       " '16_a√±os',\n",
       " '‚Äì',\n",
       " 'propietari',\n",
       " '‚Äì',\n",
       " 'propietari',\n",
       " 'inscrit',\n",
       " 'a',\n",
       " 'nombr',\n",
       " 'deb',\n",
       " 'deud',\n",
       " 'import',\n",
       " 'pag',\n",
       " 'mensual',\n",
       " 'line',\n",
       " 'defend',\n",
       " 'quien',\n",
       " 'critic',\n",
       " 'desapeg',\n",
       " 'realid',\n",
       " 'chilen',\n",
       " 'llev',\n",
       " 'visit',\n",
       " '80',\n",
       " 'comun',\n",
       " 'pod',\n",
       " 'palp',\n",
       " 'conoc',\n",
       " 'dram',\n",
       " 'neces',\n",
       " 'viviend',\n",
       " 'mil',\n",
       " 'famili',\n",
       " 'call',\n",
       " 'oficin',\n",
       " 'molest',\n",
       " 'critic',\n",
       " 'reconoc',\n",
       " 'escuch_entrev',\n",
       " 'complet',\n",
       " 'a',\n",
       " 'continu']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentences[110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T20:46:14.842693Z",
     "start_time": "2019-08-26T20:46:14.839706Z"
    }
   },
   "source": [
    "### Definir el modelo\n",
    "\n",
    "\n",
    "\n",
    "Primero, como es usual, creamos el modelo. En este caso, usaremos uno de los primero modelos de embeddings neuronales: `word2vec`\n",
    "\n",
    "Algunos par√°metros importantes:\n",
    "\n",
    "- `min_count`: Ignora todas las palabras que tengan frecuencia menor a la indicada.\n",
    "- `window` : Tama√±o de la ventana. Usaremos 4.\n",
    "- `size` : El tama√±o de los embeddings que crearemos. Por lo general, el rendimiento sube cuando se usan mas dimensiones, pero despu√©s de 300 ya no se nota cambio. Ahora, usaremos solo 200.\n",
    "- `workers`: Cantidad de CPU que ser√°n utilizadas en el entrenamiento."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec = Word2Vec(min_count = 10,\n",
    "                 window = 4,\n",
    "                 sample= 6e-5,\n",
    "                 alpha= 0.03,\n",
    "                 min_alpha= 0.0007,\n",
    "                 negative= 0,\n",
    "                 workers = multiprocessing.cpu_count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construir el vocabulario\n",
    "\n",
    "Para esto, se crear√° un conjunto que contendr√° (una sola vez) todas aquellas palabras que aparecen mas de `min_count` veces. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec.build_vocab(sentences, progress_per = 10000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T21:11:58.054500Z",
     "start_time": "2019-08-26T21:11:58.050511Z"
    }
   },
   "source": [
    "### Entrenar el Modelo\n",
    "\n",
    "A continuaci√≥n, entenaremos el modelo. \n",
    "Los par√°metros que usaremos ser√°n: \n",
    "\n",
    "- `total_examples`: N√∫mero de documentos.\n",
    "- `epochs`: N√∫mero de veces que se iterar√° sobre el corpus.\n",
    "\n",
    "Es recomendable que tengan instalado `cpython` antes de continuar. Aumenta bastante la velocidad de entrenamiento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time: 1.88 mins\n"
     ]
    }
   ],
   "source": [
    "t = time()\n",
    "w2vec.train(sentences, total_examples = w2vec.corpus_count, epochs=15, report_delay=10 )\n",
    "print('time: {} mins'.format(round((time() - t)/60,2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-6c363b0058da>:1: DeprecationWarning: Call to deprecated `init_sims` (Gensim 4.0.0 implemented internal optimizations that make calls to init_sims() unnecessary. init_sims() is now obsoleted and will be completely removed in future versions. See https://github.com/RaRe-Technologies/gensim/wiki/Migrating-from-Gensim-3.x-to-4).\n",
      "  w2vec.init_sims(replace=True)\n",
      "WARNING:gensim.models.keyedvectors:destructive init_sims(replace=True) deprecated & no longer required for space-efficiency\n"
     ]
    }
   ],
   "source": [
    "w2vec.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-26T21:43:36.571382Z",
     "start_time": "2019-08-26T21:43:36.567392Z"
    }
   },
   "source": [
    "###  Guardar y cargar el modelo\n",
    "\n",
    "Para ahorrar tiempo, usaremos un modelo preentrenado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-05-07T19:23:19.548567Z",
     "start_time": "2020-05-07T19:23:18.572531Z"
    }
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('./pretrained_models'):\n",
    "    os.mkdir('./pretrained_models')\n",
    "\n",
    "w2vec.save('./pretrained_models/w2vec_noticias.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec_2 = KeyedVectors.load('./pretrained_models/w2vec_noticias.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tasks: Palabras mas similares y Analog√≠as\n",
    "\n",
    "### Palabras mas similares\n",
    "\n",
    "Tal como dijimos anteriormente, los embeddings son capaces de codificar toda la informaci√≥n contextual de las palabras en vectores.\n",
    "\n",
    "Y como cualquier objeto matem√°tico, estos pueden operados para encontrar ciertas propiedades. Tal es el caso de las  encontrar las palabras mas similares, lo que no es mas que encontrar los n vecinos mas cercanos del vector.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<gensim.models.keyedvectors.KeyedVectors at 0x7fa8aa0ef5b0>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vec_2.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('alcaz', 0.36716562509536743),\n",
       " ('pon', 0.3508830666542053),\n",
       " ('instanci', 0.3440763056278229),\n",
       " ('hoffm', 0.33845919370651245),\n",
       " ('urban', 0.3383106589317322),\n",
       " ('vaticin', 0.33535242080688477),\n",
       " ('gratuit', 0.332356333732605),\n",
       " ('middl', 0.33211106061935425),\n",
       " ('imped', 0.3308682441711426),\n",
       " ('ido', 0.3300718069076538)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2vec_2.wv.most_similar(positive=[\"social\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "260.4px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
